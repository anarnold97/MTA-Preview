<?xml version='1.0' encoding='UTF-8'?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" class="chrometwo"><head><title>Release notes</title><link rel="stylesheet" type="text/css" href="Common_Content/css/default.css"/><meta name="generator" content="publican v4.3.2"/><meta name="description" content="This document describes new features, known issues, and resolved issues for the Migration Toolkit for Virtualization 2.6."/><link rel="next" href="#making-open-source-more-inclusive" title="Making open source more inclusive"/><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/><script type="text/javascript" src="Common_Content/scripts/jquery-1.7.1.min.js"> </script><script type="text/javascript" src="Common_Content/scripts/utils.js"> </script><script type="text/javascript" src="Common_Content/scripts/highlight.js/highlight.pack.js"> </script></head><body><div id="chrometwo"><div id="main"><div xml:lang="en-US" class="book" id="idm46144040747728"><div class="titlepage"><div><div class="producttitle"><span class="productname">Migration Toolkit for Virtualization</span> <span class="productnumber">2.6</span></div><div><h1 class="title">Release notes</h1></div><div><h2 class="subtitle">Version 2.6</h2></div><div><div xml:lang="en-US" class="authorgroup"><div class="author"><h3 class="author"><span class="firstname">Red Hat Modernization and Migration</span> <span class="surname">Documentation Team</span></h3><code class="email"><a class="email" href="mailto:ccs-mms-docs@redhat.com">ccs-mms-docs@redhat.com</a></code></div></div></div><div><a href="#idm46144045553040">Legal Notice</a></div><div><div class="abstract"><p class="title"><strong>Abstract</strong></p><div class="para">
				This document describes new features, known issues, and resolved issues for the Migration Toolkit for Virtualization 2.6.
			</div></div></div></div><hr/></div><div class="toc"><ul class="toc"><li><span class="preface"><a href="#making-open-source-more-inclusive">Making open source more inclusive</a></span></li><li><span class="chapter"><a href="#rn-26_release-notes">1. Migration Toolkit for Virtualization 2.6</a></span><ul><li><span class="section"><a href="#technical-changes-26_release-notes">1.1. Technical changes</a></span></li><li><span class="section"><a href="#new-features-and-enhancements-26_release-notes">1.2. New features and enhancements</a></span><ul><li><span class="section"><a href="#new-features-and-enhancements-26-3_release-notes">1.2.1. New features and enhancements 2.6.3</a></span></li><li><span class="section"><a href="#new-features-and-enhancements-26-0_release-notes">1.2.2. New features and enhancements 2.6.0</a></span></li></ul></li><li><span class="section"><a href="#resolved-issues-26_release-notes">1.3. Resolved issues</a></span><ul><li><span class="section"><a href="#resolved-issues-26-4_release-notes">1.3.1. Resolved issues 2.6.4</a></span></li><li><span class="section"><a href="#resolved-issues-26-3_release-notes">1.3.2. Resolved issues 2.6.3</a></span></li><li><span class="section"><a href="#resolved-issues-26-2_release-notes">1.3.3. Resolved issues 2.6.2</a></span></li><li><span class="section"><a href="#resolved-issues-26-1_release-notes">1.3.4. Resolved issues 2.6.1</a></span></li><li><span class="section"><a href="#resolved-issues-26-0_release-notes">1.3.5. Resolved issues 2.6.0</a></span></li></ul></li><li><span class="section"><a href="#known-issues-26_release-notes">1.4. Known issues</a></span></li></ul></li><li><span class="chapter"><a href="#rn-2.5_release-notes">2. Migration Toolkit for Virtualization 2.5</a></span><ul><li><span class="section"><a href="#technical-changes-25_release-notes">2.1. Technical changes</a></span></li><li><span class="section"><a href="#new-features-and-enhancements-25_release-notes">2.2. New features and enhancements</a></span></li><li><span class="section"><a href="#known-issues-25_release-notes">2.3. Known issues</a></span></li><li><span class="section"><a href="#resolved-issues-25_release-notes">2.4. Resolved issues</a></span></li><li><span class="section"><a href="#upgrade-notes-25_release-notes">2.5. Upgrade notes</a></span></li></ul></li><li><span class="chapter"><a href="#rn-24_release-notes">3. Migration Toolkit for Virtualization 2.4</a></span><ul><li><span class="section"><a href="#technical-changes-24_release-notes">3.1. Technical changes</a></span></li><li><span class="section"><a href="#new-features-and-enhancements-24_release-notes">3.2. New features and enhancements</a></span></li><li><span class="section"><a href="#known-issues-24_release-notes">3.3. Known issues</a></span></li><li><span class="section"><a href="#resolved-issues-24_release-notes">3.4. Resolved issues</a></span></li></ul></li><li><span class="chapter"><a href="#rn-23_release-notes">4. Migration Toolkit for Virtualization 2.3</a></span><ul><li><span class="section"><a href="#technical-changes-23_release-notes">4.1. Technical changes</a></span></li><li><span class="section"><a href="#new-features-and-enhancements-23_release-notes">4.2. New features and enhancements</a></span></li><li><span class="section"><a href="#known-issues-23_release-notes">4.3. Known issues</a></span></li></ul></li><li><span class="chapter"><a href="#rn-22_release-notes">5. Migration Toolkit for Virtualization 2.2</a></span><ul><li><span class="section"><a href="#technical-changes-22_release-notes">5.1. Technical changes</a></span></li><li><span class="section"><a href="#new-features-and-enhancements-22_release-notes">5.2. New features and enhancements</a></span></li><li><span class="section"><a href="#known-issues-22_release-notes">5.3. Known issues</a></span></li></ul></li><li><span class="chapter"><a href="#rn-21_release-notes">6. Migration Toolkit for Virtualization 2.1</a></span><ul><li><span class="section"><a href="#technical-changes-21_release-notes">6.1. Technical changes</a></span></li><li><span class="section"><a href="#new-features-and-enhancements-21_release-notes">6.2. New features and enhancements</a></span></li><li><span class="section"><a href="#known-issues-21_release-notes">6.3. Known issues</a></span></li></ul></li><li><span class="chapter"><a href="#rn-20_release-notes">7. Migration Toolkit for Virtualization 2.0</a></span><ul><li><span class="section"><a href="#new-features-and-enhancements-20_release-notes">7.1. New features and enhancements</a></span></li><li><span class="section"><a href="#known-issues-20_release-notes">7.2. Known issues</a></span></li></ul></li></ul></div><section class="preface" id="making-open-source-more-inclusive"><div class="titlepage"><div><div><h1 class="title">Making open source more inclusive</h1></div></div></div><p>
			Red Hat is committed to replacing problematic language in our code, documentation, and web properties. We are beginning with these four terms: master, slave, blacklist, and whitelist. Because of the enormity of this endeavor, these changes will be implemented gradually over several upcoming releases. For more details, see <a class="link" href="https://www.redhat.com/en/blog/making-open-source-more-inclusive-eradicating-problematic-language">our CTO Chris Wright’s message</a>.
		</p></section><section class="chapter" id="rn-26_release-notes"><div class="titlepage"><div><div><h1 class="title">Chapter 1. Migration Toolkit for Virtualization 2.6</h1></div></div></div><p>
			You can use the Migration Toolkit for Virtualization (MTV) to migrate virtual machines from the following source providers to OpenShift Virtualization destination providers:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					VMware vSphere
				</li><li class="listitem">
					Red Hat Virtualization (RHV)
				</li><li class="listitem">
					OpenStack
				</li><li class="listitem">
					Open Virtual Appliances (OVAs) that were created by VMware vSphere
				</li><li class="listitem">
					Remote OpenShift Virtualization clusters
				</li></ul></div><p>
			The release notes describe technical changes, new features and enhancements, known issues, and resolved issues.
		</p><section class="section" id="technical-changes-26_release-notes"><div class="titlepage"><div><div><h2 class="title">1.1. Technical changes</h2></div></div></div><p>
				This release has the following technical changes:
			</p><div class="formalpara"><p class="title"><strong>Simplified the creation of vSphere providers</strong></p><p>
					In earlier releases of MTV, users had to specify a fingerprint when creating a vSphere provider. This required users to retrieve the fingerprint from the server that vCenter runs on. MTV no longer requires this fingerprint as an input, but rather computes it from the specified certificate in the case of a secure connection or automatically retrieves it from the server that runs vCenter/ESXi in the case of an insecure connection.
				</p></div><div class="formalpara"><p class="title"><strong>Redesigned the migration plan creation dialog</strong></p><p>
					The user interface console has improved the process of creating a migration plan. The new migration plan dialog enables faster creation of migration plans.
				</p></div><p>
				It includes only the minimal settings that are required, while you can confirgure advanced settings separately. The new dialog also provides defaults for network and storage mappings, where applicable. The new dialog can also be invoked from the the <code class="literal">Provider</code> &gt; <code class="literal">Virtual Machines</code> tab, after selecting the virtual machines to migrate. It also better aligns with the user experience in the OCP console.
			</p><div class="formalpara"><p class="title"><strong><code class="literal">virtual machine preferences</code> have replaced OpenShift templates</strong></p><p>
					The <code class="literal">virtual machine preferences</code> have replaced OpenShift templates. MTV currently falls back to using OpenShift templates when a relevant preference is not available.
				</p></div><p>
				Custom mappings of guest operating system type to virtual machine preference can be configured using <code class="literal">config</code> maps. This is in order to use custom virtual machine preferences, or to support more guest operating system types.
			</p><div class="formalpara"><p class="title"><strong>Full support for migration from OVA</strong></p><p>
					Migration from OVA moves from being a Technical Preview and is now a fully supported feature.
				</p></div><div class="formalpara"><p class="title"><strong>The VM is posted with its desired <code class="literal">Running</code> state</strong></p><p>
					MTV creates the VM with its desired <code class="literal">Running</code> state on the target provider, instead of creating the VM and then running it as an additional operation. <a class="link" href="https://issues.redhat.com/browse/MTV-794">(MTV-794)</a>
				</p></div><div class="formalpara"><p class="title"><strong>The <code class="literal">must-gather</code> logs can now be loaded only by using the CLI</strong></p><p>
					The MTV web console can no longer download logs. With this update, you must download <code class="literal">must-gather</code> logs by using CLI commands. For more information, see <a class="link" href="https://github.com/openshift/must-gather-operator">Must Gather Operator</a>.
				</p></div><div class="formalpara"><p class="title"><strong>MTV no longer runs <code class="literal">pvc-init</code> pods when migrating from vSphere</strong></p><p>
					MTV no longer runs <code class="literal">pvc-init</code> pods during cold migration from a vSphere provider to the OpenShift cluster that MTV is deployed on. However, in other flows where data volumes are used, they are set with the <code class="literal">cdi.kubevirt.io/storage.bind.immediate.requested</code> annotation, and CDI runs first-consume pods for storage classes with volume binding mode <code class="literal">WaitForFirstConsumer</code>.
				</p></div></section><section class="section" id="new-features-and-enhancements-26_release-notes"><div class="titlepage"><div><div><h2 class="title">1.2. New features and enhancements</h2></div></div></div><p>
				This section provides features and enhancements introduced in Migration Toolkit for Virtualization 2.6.
			</p><section class="section" id="new-features-and-enhancements-26-3_release-notes"><div class="titlepage"><div><div><h3 class="title">1.2.1. New features and enhancements 2.6.3</h3></div></div></div><div class="formalpara"><p class="title"><strong>Support for migrating LUKS-encrypted devices in migrations from vSphere</strong></p><p>
						You can now perform cold migrations from a vSphere provider of VMs whose virtual disks are encrypted by Linux Unified Key Setup (LUKS). <a class="link" href="https://issues.redhat.com/browse/MTV-831">(MTV-831)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Specifying the primary disk when migrating from vSphere</strong></p><p>
						You can now specify the primary disk when you migrate VMs from vSphere with more than one bootable disk. This avoids MTV automatically attempting to convert the first bootable disk that it detects while it examines all the disks of a virtual machine. This feature is needed because the first bootable disk is not necessarily the disk that the VM is expected to boot from in OpenShift Virtualization. <a class="link" href="https://issues.redhat.com/browse/MTV-1079">(MTV-1079)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Links to remote provider UIs</strong></p><p>
						You can now remotely access the UI of a remote cluster when you create a source provider. For example, if the provider is a remote Red Hat Virtualization RHV cluster, MTV adds a link to the remote RHV web console when you define the provider. This feature makes it easier for you to manage and debug a migration from remote clusters. <a class="link" href="https://issues.redhat.com/browse/MTV-1054">(MTV-1054)</a>
					</p></div></section><section class="section" id="new-features-and-enhancements-26-0_release-notes"><div class="titlepage"><div><div><h3 class="title">1.2.2. New features and enhancements 2.6.0</h3></div></div></div><div class="formalpara"><p class="title"><strong>Migration from vSphere over a secure connection</strong></p><p>
						You can now specify a CA certificate that can be used to authenticate the server that runs vCenter or ESXi, depending on the specified SDK endpoint of the vSphere provider. <a class="link" href="https://issues.redhat.com/browse/MTV-530">(MTV-530)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Migration to or from a remote OpenShift over a secure connection</strong></p><p>
						You can now specify a CA certificate that can be used to authenticate the API server of a remote OpenShift cluster. <a class="link" href="https://issues.redhat.com/browse/MTV-728">(MTV-728)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Migration from an ESXi server without going through vCenter</strong></p><p>
						MTV enables the configuration of vSphere providers with the SDK of ESXi. You need to select ESXi as the Endpoint type of the vSphere provider and specify the URL of the SDK of the ESXi server. <a class="link" href="https://issues.redhat.com/browse/MTV-514">(MTV-514)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Migration of image-based VMs from OpenStack</strong></p><p>
						MTV supports the migration of VMs that were created from images in OpenStack. <a class="link" href="https://issues.redhat.com/browse/MTV-644">(MTV-644)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Migration of VMs with Fibre Channel LUNs from RHV</strong></p><p>
						MTV supports migrations of VMs that are set with Fibre Channel (FC) LUNs from RHV. As with other LUN disks, you need to ensure the OpenShift nodes have access to the FC LUNs. During the migrations, the FC LUNs are detached from the source VMs in RHV and attached to the migrated VMs in OpenShift. <a class="link" href="https://issues.redhat.com/browse/MTV-659">(MTV-659)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Preserve CPU types of VMs that are migrated from RHV</strong></p><p>
						MTV sets the CPU type of migrated VMs in OpenShift with their custom CPU type in RHV. In addition, a new option was added to migration plans that are set with RHV as a source provider to preserve the original CPU types of source VMs. When this option is selected, MTV identifies the CPU type based on the cluster configuration and sets this CPU type for the migrated VMs, for which the source VMs are not set with a custom CPU. <a class="link" href="https://issues.redhat.com/browse/MTV-547">(MTV-547)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Validation for RHEL 6 guest operating system is now available when migrating VMs with RHEL 6 guest operating system</strong></p><p>
						Red Hat Enterprise Linux (RHEL) 9 does not support RHEL 6 as a guest operating system. Therefore, RHEL 6 is not supported in OpenShift Virtualization. With this update, a validation of RHEL 6 guest operating system was added to OpenShift Virtualization. <a class="link" href="https://issues.redhat.com/browse/MTV-413">(MTV413)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Automatic retrieval of CA certificates for the provider’s URL in the console</strong></p><p>
						The ability to retrieve CA certificates, which was available in previous versions, has been restored. The <code class="literal">vSphere Verify certificate</code> option is in the <code class="literal">add-provider</code> dialog. This option was removed in the transition to the Red Hat OpenShift console and has now been added to the console. This functionality is also available for RHV, OpenStack, and OpenShift providers now. <a class="link" href="https://issues.redhat.com/browse/MTV-737">(MTV-737)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Validation of a specified VDDK image</strong></p><p>
						MTV validates the availability of a VDDK image that is specified for a vSphere provider on the target OpenShift name as part of the validation of a migration plan. MTV also checks whether the <code class="literal">libvixDiskLib.so</code> symbolic link (symlink) exists within the image. If the validation fails, the migration plan cannot be started. <a class="link" href="https://issues.redhat.com/browse/MTV-618">(MTV-618)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Add a warning and partial support for TPM</strong></p><p>
						MTV presents a warning when attempting to migrate a VM that is set with a TPM device from RHV or vSphere. The migrated VM in OpenShift would be set with a TPM device but without the content of the TPM device on the source environment. <a class="link" href="https://issues.redhat.com/browse/MTV-378">(MTV-378)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Plans that failed to migrate VMs can now be edited</strong></p><p>
						With this update, you can edit plans that have failed to migrate any VMs. Some plans fail or are canceled because of incorrect network and storage mappings. You can now edit these plans until they succeed. <a class="link" href="https://issues.redhat.com/browse/MTV-779">(MTV-779)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Validation rules are now available for OVA</strong></p><p>
						The validation service includes default validation rules for virtual machines from the Open Virtual Appliance (OVA). <a class="link" href="https://issues.redhat.com/browse/MTV-669">(MTV-669)</a>
					</p></div></section></section><section class="section" id="resolved-issues-26_release-notes"><div class="titlepage"><div><div><h2 class="title">1.3. Resolved issues</h2></div></div></div><p>
				This release has the following resolved issues:
			</p><section class="section" id="resolved-issues-26-4_release-notes"><div class="titlepage"><div><div><h3 class="title">1.3.1. Resolved issues 2.6.4</h3></div></div></div><div class="formalpara"><p class="title"><strong>Disks and drives are offline after migrating Windows virtual machines from RHV or VMware to OCP</strong></p><p>
						Windows (Windows 2022) VMs configured with multiple disks, which are <span class="strong strong"><strong>Online</strong></span> before the migration, are <span class="strong strong"><strong>Offline</strong></span> after a successful migration from RHV or VMware to Red Hat OpenShift, using MTV. Only the <code class="literal">C:\</code> primary disk is <span class="strong strong"><strong>Online</strong></span>. This issue has been resolved in MTV 2.6.4. <a class="link" href="https://issues.redhat.com/browse/MTV-1299">(MTV-1299)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Preserve IP option for Windows does not preserve all settings</strong></p><p>
						In earlier releases of MTV, while migrating a Windows 2022 Server with a static IP address assigned, and selecting the <span class="strong strong"><strong>Preserve static IPs</strong></span> option, after a successful Windows migration, while the node started and the IP address was preserved, the subnet mask, gateway, and DNS servers were not preserved. This resulted in an incomplete migration, and the customer was forced to log in locally from the console to fully configure the network. This issue has been resolved in MTV 2.6.4. <a class="link" href="https://issues.redhat.com/browse/MTV-1286">(MTV-1286)</a>
					</p></div><div class="formalpara"><p class="title"><strong><code class="literal">qemu-guest-agent</code> not being installed at first boot in Windows Server 2022</strong></p><p>
						After a successful Windows 2022 server guest migration using MTV 2.6.1, the <code class="literal">qemu-guest-agent</code> is not completely installed. The Windows Scheduled task is being created, however it is being set to run 4 hours in the future instead of the intended 2 minutes in the future. <a class="link" href="https://issues.redhat.com/browse/MTV-1325">(MTV-1325)</a>
					</p></div></section><section class="section" id="resolved-issues-26-3_release-notes"><div class="titlepage"><div><div><h3 class="title">1.3.2. Resolved issues 2.6.3</h3></div></div></div><div class="formalpara"><p class="title"><strong>CVE-2024-24788: <code class="literal">golang: net</code> malformed DNS message can cause infinite loop</strong></p><p>
						In earlier releases of MTV, there was a flaw was discovered in the <code class="literal">stdlib</code> package of the Go programming language, which impacts previous versions of MTV. This vulnerability primarily threatens web-facing applications and services that rely on Go for DNS queries. This issue has been resolved in MTV 2.6.3.
					</p></div><p>
					For more details, see <a class="link" href="https://access.redhat.com/security/cve/cve-2024-24788">(CVE-2024-24788)</a>.
				</p><div class="formalpara"><p class="title"><strong>Migration scheduling does not take into account that <code class="literal">virt-v2v</code> copies disks sequentially (vSphere only)</strong></p><p>
						In earlier releases of MTV, there was a problem with the way MTV interpreted the <code class="literal">controller_max_vm_inflight</code> setting for vSphere to schedule migrations. This issue has been resolved in MTV 2.6.3. <a class="link" href="https://issues.redhat.com/browse/MTV-1191">(MTV-1191)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Cold migrations fail after changing the ESXi network (vSphere only)</strong></p><p>
						In earlier versions of MTV, cold migrations from a vSphere provider with an ESXi SDK endpoint failed if any network was used except for the default network for disk transfers. This issue has been resolved in MTV 2.6.3. <a class="link" href="https://issues.redhat.com/browse/MTV-1180">(MTV-1180)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Warm migrations over an ESXi network are stuck in <code class="literal">DiskTransfer</code> state (vSphere only)</strong></p><p>
						In earlier versions of MTV, warm migrations over an ESXi network from a vSphere provider with a vCenter SDK endpoint were stuck in <code class="literal">DiskTransfer</code> state because MTV was unable to locate image snapshots. This issue has been resolved in MTV 2.6.3. <a class="link" href="https://issues.redhat.com/browse/MTV-1161">(MTV-1161)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Leftover PVCs are in <code class="literal">Lost</code> state after cold migrations</strong></p><p>
						In earlier versions of MTV, after cold migrations, there were leftover PVCs that had a status of <code class="literal">Lost</code> instead of being deleted, even after the migration plan that created them was archived and deleted. Investigation showed that this was because importer pods were retained after copying, by default, rather than in only specific cases. This issue has been resolved in MTV 2.6.3. <a class="link" href="https://issues.redhat.com/browse/MTV-1095">(MTV-1095)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Guest operating system from vSphere might be missing (vSphere only)</strong></p><p>
						In earlier versions of MTV, some VMs that were imported from vSphere were not mapped to a template in OpenShift while other VMs, with the same guest operating system, were mapped to the corresponding template. Investigations indicated that this was because vSphere stopped reporting the operating system after not receiving updates from VMware tools for some time. This issue has been resolved in MTV 2.6.3 by taking the value of the operating system from the output of the investigation that <code class="literal">virt-v2v</code> performs on the disks. <a class="link" href="https://issues.redhat.com/browse/MTV-1046">(MTV-1046)</a>
					</p></div></section><section class="section" id="resolved-issues-26-2_release-notes"><div class="titlepage"><div><div><h3 class="title">1.3.3. Resolved issues 2.6.2</h3></div></div></div><div class="formalpara"><p class="title"><strong>CVE-2023-45288: Golang <code class="literal">net/http, x/net/http2</code>: unlimited number of <code class="literal">CONTINUATION</code> frames can cause a denial-of-service (DoS) attack</strong></p><p>
						A flaw was discovered with the implementation of the <code class="literal">HTTP/2</code> protocol in the Go programming language, which impacts previous versions of MTV. There were insufficient limitations on the number of CONTINUATION frames sent within a single stream. An attacker could potentially exploit this to cause a denial-of-service (DoS) attack. This flaw has been resolved in MTV 2.6.2.
					</p></div><p>
					For more details, see <a class="link" href="https://access.redhat.com/security/cve/cve-2023-45288">(CVE-2023-45288)</a>.
				</p><div class="formalpara"><p class="title"><strong>CVE-2024-24785: <code class="literal">mtv-api-container</code>: Golang <code class="literal">html/template: errors</code> returned from <code class="literal">MarshalJSON</code> methods may break template escaping</strong></p><p>
						A flaw was found in the <code class="literal">html/template</code> Golang standard library package, which impacts previous versions of MTV. If errors returned from <code class="literal">MarshalJSON</code> methods contain user-controlled data, they may be used to break the contextual auto-escaping behavior of the HTML/template package, allowing subsequent actions to inject unexpected content into the templates. This flaw has been resolved in MTV 2.6.2.
					</p></div><p>
					For more details, see <a class="link" href="https://access.redhat.com/security/cve/cve-2024-24785">(CVE-2024-24785)</a>.
				</p><div class="formalpara"><p class="title"><strong>CVE-2024-24784: <code class="literal">mtv-validation-container</code>: Golang <code class="literal">net/mail</code>: comments in display names are incorrectly handled</strong></p><p>
						A flaw was found in the <code class="literal">net/mail</code> Golang standard library package, which impacts previous versions of MTV. The <code class="literal">ParseAddressList</code> function incorrectly handles comments, text in parentheses, and display names. As this is a misalignment with conforming address parsers, it can result in different trust decisions being made by programs using different parsers. This flaw has been resolved in MTV 2.6.2.
					</p></div><p>
					For more details, see <a class="link" href="https://access.redhat.com/security/cve/cve-2024-24784">(CVE-2024-24784)</a>.
				</p><div class="formalpara"><p class="title"><strong>CVE-2024-24783: <code class="literal">mtv-api-container</code>: Golang <code class="literal">crypto/x509</code>: Verify panics on certificates with an unknown public key algorithm</strong></p><p>
						A flaw was found in the <code class="literal">crypto/x509</code> Golang standard library package, which impacts previous versions of MTV. Verifying a certificate chain that contains a certificate with an unknown public key algorithm causes <code class="literal">Certificate.Verify</code> to panic. This affects all <code class="literal">crypto/tls</code> clients and servers that set <code class="literal">Config.ClientAuth</code> to <code class="literal">VerifyClientCertIfGiven</code> or <code class="literal">RequireAndVerifyClientCert</code>. The default behavior is for TLS servers to not verify client certificates. This flaw has been resolved in MTV 2.6.2.
					</p></div><p>
					For more details, see <a class="link" href="https://access.redhat.com/security/cve/cve-2024-24783">(CVE-2024-24783)</a>.
				</p><div class="formalpara"><p class="title"><strong>CVE-2023-45290: <code class="literal">mtv-api-container</code>: Golang <code class="literal">net/http</code> memory exhaustion in <code class="literal">Request.ParseMultipartForm</code></strong></p><p>
						A flaw was found in the <code class="literal">net/http</code> Golang standard library package, which impacts previous versions of MTV. When parsing a <code class="literal">multipart</code> form, either explicitly with <code class="literal">Request.ParseMultipartForm</code> or implicitly with <code class="literal">Request.FormValue</code>, <code class="literal">Request.PostFormValue</code>, or <code class="literal">Request.FormFile</code>, limits on the total size of the parsed form are not applied to the memory consumed while reading a single form line. This permits a maliciously crafted input containing long lines to cause the allocation of arbitrarily large amounts of memory, potentially leading to memory exhaustion. This flaw has been resolved in MTV 2.6.2.
					</p></div><p>
					For more details, see <a class="link" href="https://access.redhat.com/security/cve/cve-2023-45290">(CVE-2023-45290)</a>.
				</p><div class="formalpara"><p class="title"><strong>ImageConversion does not run when target storage is set with WaitForFirstConsumer (WFFC)</strong></p><p>
						In earlier releases of MTV, migration of VMs failed because the migration was stuck in the <code class="literal">AllocateDisks</code> phase. As a result of being stuck, the migration did not progress, and PVCs were not bound. The root cause of the issue was that <code class="literal">ImageConversion</code> did not run when target storage was set for <code class="literal">wait-for-first-consumer</code>. The problem was resolved in MTV 2.6.2. <a class="link" href="https://issues.redhat.com/browse/MTV-1126">(MTV-1126)</a>
					</p></div><div class="formalpara"><p class="title"><strong>forklift-controller panics when importing VMs with direct LUNs</strong></p><p>
						In earlier releases of MTV, <code class="literal">forklift-controller</code> panicked when a user attempted to import VMs that had direct LUNs. The problem was resolved in MTV 2.6.2. <a class="link" href="https://issues.redhat.com/browse/MTV-1134">(MTV-1134)</a>
					</p></div></section><section class="section" id="resolved-issues-26-1_release-notes"><div class="titlepage"><div><div><h3 class="title">1.3.4. Resolved issues 2.6.1</h3></div></div></div><div class="formalpara"><p class="title"><strong>VMs with multiple disks that are migrated from vSphere and OVA files are not being fully copied</strong></p><p>
						In MTV 2.6.0, there was a problem in copying VMs with multiple disks from VMware vSphere and from OVA files. The migrations appeared to succeed but all the disks were transferred to the same PV in the target environment while other disks were empty. In some cases, bootable disks were overridden, so the VM could not boot. In other cases, data from the other disks was missing. The problem was resolved in MTV 2.6.1. <a class="link" href="https://issues.redhat.com/browse/MTV-1067">(MTV-1067)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Migrating VMs from one Red Hat OpenShift cluster to another fails due to a timeout</strong></p><p>
						In MTV 2.6.0, migrations from one Red Hat OpenShift cluster to another failed when the time to transfer the disks of a VM exceeded the time to live (TTL) of the Export API in OpenShift, which was set to 2 hours by default. The problem was resolved in MTV 2.6.1 by setting the default TTL of the Export API to 12 hours, which greatly reduces the possibility of an expiration of the Export API. Additionally, you can increase or decrease the TTL setting as needed. <a class="link" href="https://issues.redhat.com/browse/MTV-1052">(MTV-1052)</a>
					</p></div><div class="formalpara"><p class="title"><strong>MTV forklift-controller pod crashes when receiving a disk without a datastore</strong></p><p>
						In earlier releases of MTV, if a VM was configured with a disk that was on a datastore that was no longer available in vSphere at the time a migration was attempted, the <code class="literal">forklift-controller</code> crashed, rendering MTV unusable. In MTV 2.6.1, MTV presents a critical validation for VMs with such disks, informing users of the problem, and the <code class="literal">forklift-controller</code> no longer crashes, although it cannot transfer the disk. <a class="link" href="https://issues.redhat.com/browse/MTV-1029">(MTV-1029)</a>
					</p></div></section><section class="section" id="resolved-issues-26-0_release-notes"><div class="titlepage"><div><div><h3 class="title">1.3.5. Resolved issues 2.6.0</h3></div></div></div><div class="formalpara"><p class="title"><strong>Deleting an OVA provider automatically also deletes the PV</strong></p><p>
						In earlier releases of MTV, the PV was not removed when the OVA provider was deleted. This has been resolved in MTV 2.6.0, and the PV is automatically deleted when the OVA provider is deleted. <a class="link" href="https://issues.redhat.com/browse/MTV-848">(MTV-848)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Fix for data being lost when migrating VMware VMs with snapshots</strong></p><p>
						In earlier releases of MTV, when migrating a VM that has a snapshot from VMware, the VM that was created in OpenShift Virtualization contained the data in the snapshot but not the latest data of the VM. This has been resolved in MTV 2.6.0. <a class="link" href="https://issues.redhat.com/browse/MTV-447">(MTV-447)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Canceling and deleting a failed migration plan does not clean up the <code class="literal">populate</code> pods and PVC</strong></p><p>
						In earlier releases of MTV, when you canceled and deleted a failed migration plan, and after creating a PVC and spawning the <code class="literal">populate</code> pods, the <code class="literal">populate</code> pods and PVC were not deleted. You had to delete the pods and PVC manually. This issue has been resolved in MTV 2.6.0. <a class="link" href="https://issues.redhat.com/browse/MTV-678">(MTV-678)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Red Hat OpenShift to Red Hat OpenShift migrations require the cluster version to be 4.13 or later</strong></p><p>
						In earlier releases of MTV, when migrating from Red Hat OpenShift to Red Hat OpenShift, the version of the source provider cluster had to be Red Hat OpenShift version 4.13 or later. This issue has been resolved in MTV 2.6.0, with validation being shown when migrating from versions of OpenShift before 4.13. <a class="link" href="https://issues.redhat.com/browse/MTV-734">(MTV-734)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Multiple storage domains from RHV were always mapped to a single storage class</strong></p><p>
						In earlier releases of MTV, multiple disks from different storage domains were always mapped to a single storage class, regardless of the storage mapping that was configured. This issue has been resolved in MTV 2.6.0. <a class="link" href="https://issues.redhat.com/browse/MTV-1008">(MTV-1008)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Firmware detection by virt-v2v</strong></p><p>
						In earlier releases of MTV, a VM that was migrated from an OVA that did not include the firmware type in its OVF configuration was set with UEFI. This was incorrect for VMs that were configured with BIOS. This issue has been resolved in MTV 2.6.0, as MTV now consumes the firmware that is detected by <code class="literal">virt-v2v</code> during the conversion of the disks. <a class="link" href="https://issues.redhat.com/browse/MTV-759">(MTV-759)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Creating a host secret requires validation of the secret before creation of the host</strong></p><p>
						In earlier releases of MTV, when configuring a transfer network for vSphere hosts, the console plugin created the <code class="literal">Host</code> CR before creating its secret. The secret should be specified first in order to validate it before the <code class="literal">Host</code> CR is posted. This issue has been resolved in MTV 2.6.0. <a class="link" href="https://issues.redhat.com/browse/MTV-868">(MTV-868)</a>
					</p></div><div class="formalpara"><p class="title"><strong>When adding OVA provider a <code class="literal">ConnectionTestFailed</code> message appears</strong></p><p>
						In earlier releases of MTV, when adding an OVA provider, the error message <code class="literal">ConnectionTestFailed</code> instantly appeared, although the provider had been created successfully. This issue has been resolved in MTV 2.6.0. <a class="link" href="https://issues.redhat.com/browse/MTV-671">(MTV-671)</a>
					</p></div><div class="formalpara"><p class="title"><strong>RHV provider <code class="literal">ConnectionTestSucceeded</code> True response from the wrong URL</strong></p><p>
						In earlier releases of MTV, the <code class="literal">ConnectionTestSucceeded</code> condition was set to <code class="literal">True</code> even when the URL was different than the API endpoint for the RHV Manager. This issue has been resolved in MTV 2.6.0. <a class="link" href="https://issues.redhat.com/browse/MTV-740">(MTV-740)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Migration does not fail when a vSphere Data Center is nested inside a folder</strong></p><p>
						In earlier releases of MTV, migrating a VM that is placed in a Data Center that is stored directly under the <code class="literal">/vcenter</code> in vSphere succeeded. However, it failed when the Data Center was stored inside a folder. This issue was resolved in MTV 2.6.0. <a class="link" href="https://issues.redhat.com/browse/MTV-796">(MTV-796)</a>
					</p></div><div class="formalpara"><p class="title"><strong>The OVA inventory watcher detects deleted files</strong></p><p>
						The OVA inventory watcher detects files changes, including deleted files. Updates from the <code class="literal">ova-provider-server</code> pod are now sent every five minutes to the <code class="literal">forklift-controller</code> pod that updates the inventory. <a class="link" href="https://issues.redhat.com/browse/MTV-733">(MTV-733)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Unclear error message when Forklift fails to build or create a PVC</strong></p><p>
						In earlier releases of MTV, the error logs lacked clear information to identify the reason for a failure to create a PV on a destination storage class that does not have a configured storage profile. This issue was resolved in MTV 2.6.0. <a class="link" href="https://issues.redhat.com/browse/MTV-928">(MTV-928)</a>
					</p></div><div class="formalpara"><p class="title"><strong>Plans stay indefinitely in the <code class="literal">CopyDisks</code> phase when there is an outdated ovirtvolumepopulator</strong></p><p>
						In earlier releases of MTV, an earlier failed migration could have left an outdated <code class="literal">ovirtvolumepopulator</code>. When starting a new plan for the same VM to the same project, the <code class="literal">CreateDataVolumes</code> phase did not create populator PVCs when transitioning to <code class="literal">CopyDisks</code>, causing the <code class="literal">CopyDisks</code> phase to stay indefinitely. This issue was resolved in MTV 2.6.0. <a class="link" href="https://issues.redhat.com/browse/MTV-929">(MTV-929)</a>
					</p></div><p>
					For a complete list of all resolved issues in this release, see the list of <a class="link" href="https://issues.redhat.com/issues/?filter=12436210">Resolved Issues</a> in Jira.
				</p></section></section><section class="section" id="known-issues-26_release-notes"><div class="titlepage"><div><div><h2 class="title">1.4. Known issues</h2></div></div></div><p>
				This release has the following known issues:
			</p><div class="formalpara"><p class="title"><strong>Dynamic disks are offline in Windows Server 2022 after migration from vSphere to CNV with <code class="literal">ceph-rbd</code></strong></p><p>
					The dynamic disks are <span class="strong strong"><strong>Offline</strong></span> in Windows Server 2022 after cold and warm migrations from vSphere to container-native virtualization (CNV) with Ceph RADOS Block Devices (RBD), using the storage class <code class="literal">ocs-storagecluster-ceph-rbd</code>. <a class="link" href="https://issues.redhat.com/browse/MTV-1344">(MTV-1344)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Unclear error status message for VM with no operating system</strong></p><p>
					The error status message for a VM with no operating system on the <span class="strong strong"><strong>Plans</strong></span> page of the web console does not describe the reason for the failure. <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2008846">(BZ#22008846)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Migration of virtual machines with encrypted partitions fails during a conversion (vSphere only)</strong></p><p>
					vSphere only: Migrations from RHV and OpenStack do not fail, but the encryption key might be missing on the target Red Hat OpenShift cluster.
				</p></div><div class="formalpara"><p class="title"><strong>Migration fails during precopy/cutover while performing a snapshot operation on the source VM</strong></p><p>
					Warm migration from RHV fails if a snapshot operation is triggered and running on the source VM at the same time as the migration is scheduled. The migration does not wait for the snapshot operation to finish. <a class="link" href="https://issues.redhat.com/browse/MTV-456">(MTV-456)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Unable to schedule migrated VM with multiple disks to more than one storage class of type <code class="literal">hostPath</code></strong></p><p>
					When migrating a VM with multiple disks to more than one storage class of type <code class="literal">hostPath</code>, it might happen that a VM cannot be scheduled. Workaround: Use shared storage on the target Red Hat OpenShift cluster.
				</p></div><div class="formalpara"><p class="title"><strong>Non-supported guest operating systems in warm migrations</strong></p><p>
					Warm migrations and migrations to remote Red Hat OpenShift clusters from vSphere do not support the same guest operating systems that are supported in cold migrations and migrations to the local Red Hat OpenShift cluster. RHEL 8 and RHEL 9 might cause this limitation.
				</p></div><p>
				See <a class="link" href="https://access.redhat.com/articles/1351473">Converting virtual machines from other hypervisors to KVM with virt-v2v in RHEL 7, RHEL 8, and RHEL 9</a> for the list of supported guest operating systems.
			</p><div class="formalpara"><p class="title"><strong>VMs from vSphere with RHEL 9 guest operating system can start with network interfaces that are down</strong></p><p>
					When migrating VMs that are installed with RHEL 9 as a guest operating system from vSphere, the network interfaces of the VMs could be disabled when they start in OpenShift Virtualization. <a class="link" href="https://issues.redhat.com/browse/MTV-491">(MTV-491)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Migration of a VM with NVME disks from vSphere fails</strong></p><p>
					When migrating a virtual machine (VM) with NVME disks from vSphere, the migration process fails, and the Web Console shows that the <code class="literal">Convert image to kubevirt</code> stage is <code class="literal">running</code> but did not finish successfully. <a class="link" href="https://issues.redhat.com/browse/MTV-963">(MTV-963)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Importing image-based VMs can fail</strong></p><p>
					Migrating an image-based VM without the <code class="literal">virtual_size</code> field can fail on a block mode storage class. <a class="link" href="https://issues.redhat.com/browse/MTV-946">(MTV-946)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Deleting a migration plan does not remove temporary resources</strong></p><p>
					Deleting a migration plan does not remove temporary resources such as importer pods, conversion pods, config maps, secrets, failed VMs, and data volumes. You must archive a migration plan before deleting it to clean up the temporary resources. <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2018974">(BZ#2018974)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Migrating VMs with independent persistent disks from VMware to OCP-V fails</strong></p><p>
					Migrating VMs with independent persistent disks from VMware to OCP-V fails. <a class="link" href="https://issues.redhat.com/browse/MTV-993">(MTV-993)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Guest operating system from vSphere might be missing</strong></p><p>
					When vSphere does not receive updates about the guest operating system from the VMware tools, it considers the information about the guest operating system to be outdated and ceases to report it. When this occurs, MTV is unaware of the guest operating system of the VM and is unable to associate it with the appropriate virtual machine preference or OpenShift template. <a class="link" href="https://issues.redhat.com/browse/MTV-1046">(MTV-1046)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Failure to migrate an image-based VM from OpenStack to the <code class="literal">default</code> project</strong></p><p>
					The migration process fails when migrating an image-based VM from OpenStack to the <code class="literal">default</code> project. <a class="link" href="https://issues.redhat.com/browse/MTV-964">(MTV-964)</a>
				</p></div><p>
				For a complete list of all known issues in this release, see the list of <a class="link" href="https://issues.redhat.com/issues/?filter=12436209">Known Issues</a> in Jira.
			</p></section></section><section class="chapter" id="rn-2.5_release-notes"><div class="titlepage"><div><div><h1 class="title">Chapter 2. Migration Toolkit for Virtualization 2.5</h1></div></div></div><p>
			You can use the Migration Toolkit for Virtualization (MTV) to migrate virtual machines from the following source providers to OpenShift Virtualization destination providers:
		</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
					VMware vSphere
				</li><li class="listitem">
					Red Hat Virtualization (RHV)
				</li><li class="listitem">
					OpenStack
				</li><li class="listitem">
					Open Virtual Appliances (OVAs) that were created by VMware vSphere
				</li><li class="listitem">
					Remote OpenShift Virtualization clusters
				</li></ul></div><p>
			The release notes describe technical changes, new features and enhancements, and known issues for Migration Toolkit for Virtualization.
		</p><section class="section" id="technical-changes-25_release-notes"><div class="titlepage"><div><div><h2 class="title">2.1. Technical changes</h2></div></div></div><p>
				This release has the following technical changes:
			</p><div class="formalpara"><p class="title"><strong>Migration from OpenStack moves to being a fully supported feature</strong></p><p>
					In this version of MTV, migration using OpenStack source providers graduated from a Technology Preview feature to a fully supported feature.
				</p></div><div class="formalpara"><p class="title"><strong>Disabling FIPS</strong></p><p>
					MTV enables migrations from vSphere source providers by not enforcing Enterprise Master Secret (EMS). This enables migrating from all vSphere versions that MTV supports, including migrations that do not meet 2023 FIPS requirements.
				</p></div><div class="formalpara"><p class="title"><strong>Integration of the create and update provider user interface</strong></p><p>
					The user interface of the create and update providers now aligns with the look and feel of the Red Hat OpenShift web console and displays up-to-date data.
				</p></div><div class="formalpara"><p class="title"><strong>Standalone UI</strong></p><p>
					The old UI of MTV 2.3 cannot be enabled by setting <code class="literal">feature_ui: true</code> in ForkliftController anymore.
				</p></div><div class="formalpara"><p class="title"><strong>Support deployment on OpenShift 4.15</strong></p><p>
					MTV 2.5.6 can be deployed on OpenShift 4.15 clusters.
				</p></div></section><section class="section" id="new-features-and-enhancements-25_release-notes"><div class="titlepage"><div><div><h2 class="title">2.2. New features and enhancements</h2></div></div></div><p>
				This release has the following features and improvements:
			</p><div class="formalpara"><p class="title"><strong>Migration of OVA files from VMware vSphere</strong></p><p>
					In MTV 2.6, you can migrate using Open Virtual Appliance (OVA) files that were created by VMware vSphere as source providers. <a class="link" href="https://issues.redhat.com/browse/MTV-336">(MTV-336)</a>
				</p></div><div class="admonition note"><div class="admonition_header">Note</div><div><p>
					Migration of OVA files that were not created by VMware vSphere but are compatible with vSphere might succeed. However, migration of such files is not supported by MTV. MTV supports only OVA files created by VMware vSphere.
				</p></div></div><p>
				Migration using one or more Open Virtual Appliance (OVA) files as a source provider is a Technology Preview.
			</p><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					Migration using one or more Open Virtual Appliance (OVA) files as a source provider is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
				</p><p>
					For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">https://access.redhat.com/support/offerings/techpreview/</a>.
				</p></div></div><div class="formalpara"><p class="title"><strong>Migrating VMs between Red Hat OpenShift clusters</strong></p><p>
					In MTV 2.6, you can now use Red Hat OpenShift Virtualization provider as a source provider and a destination provider. You can migrate VMs from the cluster that MTV is deployed on to another cluster, or from a remote cluster to the cluster that MTV is deployed on. <a class="link" href="https://issues.redhat.com/browse/MTV-571">(MTV-571)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Migration of VMs with direct LUNs from RHV</strong></p><p>
					During the migration from Red Hat Virtualization (RHV), direct Logical Units (LUNs) are detached from the source virtual machines and attached to the target virtual machines. Note that this mechanism does not work yet for Fibre Channel. <a class="link" href="https://issues.redhat.com/browse/MTV-329">(MTV-329)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Additional authentication methods for OpenStack</strong></p><p>
					In addition to standard password authentication, MTV supports the following authentication methods: Token authentication and Application credential authentication. <a class="link" href="https://issues.redhat.com/browse/MTV-539">(MTV-539)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Validation rules for OpenStack</strong></p><p>
					The validation service includes default validation rules for virtual machines from OpenStack. <a class="link" href="https://issues.redhat.com/browse/MTV-508">(MTV-508)</a>
				</p></div><div class="formalpara"><p class="title"><strong>VDDK is now optional for VMware vSphere providers</strong></p><p>
					You can now create the VMware vSphere source provider without specifying a VMware Virtual Disk Development Kit (VDDK) <code class="literal">init</code> image. It is strongly recommended you create a VDDK <code class="literal">init</code> image to accelerate migrations.
				</p></div><div class="formalpara"><p class="title"><strong>Deployment on OKE enabled</strong></p><p>
					In MTV 2.5.3, deployment on OpenShift Kubernetes Engine (OKE) has been enabled. For more information, see <a class="link" href="https://docs.openshift.com/container-platform/4.15/welcome/oke_about.html">About OpenShift Kubernetes Engine</a>. <a class="link" href="https://issues.redhat.com/browse/MTV-803">(MTV-803)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Migration of VMs to destination storage classes with encrypted RBD now supported</strong></p><p>
					In MTV 2.5.4, migration of VMs to destination storage classes that have encrypted RADOS Block Devices (RBD) volumes is now supported.
				</p></div><p>
				To make use of this new feature, set the value of the parameter <code class="literal">controller_block_overhead</code> to <code class="literal">1Gi</code>, following the procedure in <a class="link" href="https://access.redhat.com/documentation/en-us/migration_toolkit_for_virtualization/2.5/html-single/installing_and_using_the_migration_toolkit_for_virtualization/index#configuring-mtv-operator_mtv">Configuring the MTV Operator</a>. <a class="link" href="https://issues.redhat.com/browse/MTV-851">(MTV-851)</a>
			</p></section><section class="section" id="known-issues-25_release-notes"><div class="titlepage"><div><div><h2 class="title">2.3. Known issues</h2></div></div></div><p>
				This release has the following known issues:
			</p><div class="formalpara"><p class="title"><strong>Deleting migration plan does not remove temporary resources</strong></p><p>
					Deleting a migration plan does not remove temporary resources such as importer pods, conversion pods, config maps, secrets, failed VMs and data volumes. You must archive a migration plan before deleting it to clean up the temporary resources. <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2018974">(BZ#2018974)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Unclear error status message for VM with no operating system</strong></p><p>
					The error status message for a VM with no operating system on the <span class="strong strong"><strong>Plans</strong></span> page of the web console does not describe the reason for the failure. <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2008846">(BZ#22008846)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Migration of virtual machines with encrypted partitions fails during conversion</strong></p><p>
					vSphere only: Migrations from RHV and OpenStack do not fail, but the encryption key may be missing on the target Red Hat OpenShift cluster.
				</p></div><div class="formalpara"><p class="title"><strong>Migration fails during precopy/cutover while performing a snapshot operation on the source VM</strong></p><p>
					Warm migration from RHV fails if a snapshot operation is triggered and running on the source VM at the same time as the migration is scheduled. The migration does not wait for the snapshot operation to finish. <a class="link" href="https://issues.redhat.com/browse/MTV-456">(MTV-456)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Unable to schedule migrated VM with multiple disks to more than one storage classes of type hostPath</strong></p><p>
					When migrating a VM with multiple disks to more than one storage classes of type <code class="literal">hostPath</code>, it might happen that a VM cannot be scheduled. Workaround: Use shared storage on the target Red Hat OpenShift cluster.
				</p></div><div class="formalpara"><p class="title"><strong>Non-supported guest operating systems in warm migrations</strong></p><p>
					Warm migrations and migrations to remote Red Hat OpenShift clusters from vSphere do not support all types of guest operating systems that are supported in cold migrations to the local Red Hat OpenShift cluster. This is a consequence of using RHEL 8 in the former case and RHEL 9 in the latter case.<br/> See <a class="link" href="https://access.redhat.com/articles/1351473">Converting virtual machines from other hypervisors to KVM with virt-v2v in RHEL 7, RHEL 8, and RHEL 9</a> for the list of supported guest operating systems.
				</p></div><div class="formalpara"><p class="title"><strong>VMs from vSphere with RHEL 9 guest operating system can start with network interfaces that are down</strong></p><p>
					When migrating VMs that are installed with RHEL 9 as guest operating system from vSphere, the network interfaces of the VMs could be disabled when they start in OpenShift Virtualization. <a class="link" href="https://issues.redhat.com/browse/MTV-491">(MTV-491)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Import OVA: ConnectionTestFailed message appears when adding OVA provider</strong></p><p>
					When adding an OVA provider, the error message <code class="literal">ConnectionTestFailed</code> can appear, although the provider is created successfully. If the message does not disappear after a few minutes and the provider status does not move to <code class="literal">Ready</code>, this means that the <code class="literal">ova server pod creation</code> has failed. <a class="link" href="https://issues.redhat.com/browse/MTV-671">(MTV-671)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Left over <code class="literal">ovirtvolumepopulator</code> from failed migration causes plan to stay indefinitely in <code class="literal">CopyDisks</code> phase</strong></p><p>
					An outdated <code class="literal">ovirtvolumepopulator</code> in the namespace, left over from an earlier failed migration, stops a new plan of the same VM when it transitions to <code class="literal">CopyDisks</code> phase. The plan remains in that phase indefinitely. <a class="link" href="https://issues.redhat.com/browse/MTV-929">(MTV-929)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Unclear error message when Forklift fails to build a PVC</strong></p><p>
					The migration fails to build the Persistent Volume Claim (PVC) if the destination storage class does not have a configured storage profile. The <code class="literal">forklift-controller</code> raises an error message without a clear reason for failing to create a PVC. <a class="link" href="https://issues.redhat.com/browse/MTV-928">(MTV-928)</a>
				</p></div><p>
				For a complete list of all known issues in this release, see the list of <a class="link" href="https://issues.redhat.com/issues/?filter=12430968">Known Issues</a> in Jira.
			</p></section><section class="section" id="resolved-issues-25_release-notes"><div class="titlepage"><div><div><h2 class="title">2.4. Resolved issues</h2></div></div></div><p>
				This release has the following resolved issues:
			</p><div class="formalpara"><p class="title"><strong>Flaw was found in jsrsasign package which is vulnerable to Observable Discrepancy</strong></p><p>
					Versions of the package <code class="literal">jsrsasign</code> before 11.0.0, used in earlier releases of MTV, are vulnerable to Observable Discrepancy in the RSA PKCS1.5 or RSA-OAEP decryption process. This discrepancy means an attacker could decrypt ciphertexts by exploiting this vulnerability. However, exploiting this vulnerability requires the attacker to have access to a large number of ciphertexts encrypted with the same key. This issue has been resolved in MTV 2.5.5 by upgrading the package <code class="literal">jsrasign</code> to version 11.0.0.
				</p></div><p>
				For more information, see <a class="link" href="https://access.redhat.com/security/cve/CVE-2024-21484">CVE-2024-21484</a>.
			</p><div class="formalpara"><p class="title"><strong>Multiple HTTP/2 enabled web servers are vulnerable to a DDoS attack (Rapid Reset Attack)</strong></p><p>
					A flaw was found in handling multiplexed streams in the HTTP/2 protocol. In previous releases of MTV, the HTTP/2 protocol allowed a denial of service (server resource consumption) because request cancellation could reset multiple streams quickly. The server had to set up and tear down the streams while not hitting any server-side limit for the maximum number of active streams per connection, which resulted in a denial of service due to server resource consumption.
				</p></div><p>
				This issue has been resolved in MTV 2.5.2. It is advised to update to this version of MTV or later.
			</p><p>
				For more information, see <a class="link" href="https://access.redhat.com/security/cve/cve-2023-44487">CVE-2023-44487 (Rapid Reset Attack)</a> and <a class="link" href="https://access.redhat.com/security/cve/cve-2023-39325">CVE-2023-39325 (Rapid Reset Attack)</a>.
			</p><div class="formalpara"><p class="title"><strong>Gin Web Framework does not properly sanitize filename parameter of <code class="literal">Context.FileAttachment</code> function</strong></p><p>
					A flaw was found in the Gin-Gonic Gin Web Framework, used by MTV. The filename parameter of the <code class="literal">Context.FileAttachment</code> function was not properly sanitized. This flaw in the package could allow a remote attacker to bypass security restrictions caused by improper input validation by the filename parameter of the <code class="literal">Context.FileAttachment</code> function. A maliciously created filename could cause the <code class="literal">Content-Disposition</code> header to be sent with an unexpected filename value, or otherwise modify the <code class="literal">Content-Disposition</code> header.
				</p></div><p>
				This issue has been resolved in MTV 2.5.2. It is advised to update to this version of MTV or later.
			</p><p>
				For more information, see <a class="link" href="https://access.redhat.com/security/cve/cve-2023-29401">CVE-2023-29401 (Gin-Gonic Gin Web Framework)</a> and <a class="link" href="https://access.redhat.com/security/cve/CVE-2023-26125">CVE-2023-26125</a>.
			</p><div class="formalpara"><p class="title"><strong>CVE-2023-26144: mtv-console-plugin-container: graphql: Insufficient checks in the OverlappingFieldsCanBeMergedRule.ts</strong></p><p>
					A flaw was found in the package GraphQL from 16.3.0 and before 16.8.1. This flaw means MTV versions before MTV 2.5.2 are vulnerable to Denial of Service (DoS) due to insufficient checks in the <code class="literal">OverlappingFieldsCanBeMergedRule.ts</code> file when parsing large queries. This issue may allow an attacker to degrade system performance. <a class="link" href="https://issues.redhat.com/browse/MTV-712">(MTV-712)</a>
				</p></div><p>
				This issue has been resolved in MTV 2.5.2. It is advised to update to this version of MTV or later.
			</p><p>
				For more information, see <a class="link" href="https://access.redhat.com/security/cve/CVE-2023-26144">CVE-2023-26144</a>.
			</p><div class="formalpara"><p class="title"><strong>CVE-2023-45142: Memory leak found in the otelhttp handler of open-telemetry</strong></p><p>
					A flaw was found in <code class="literal">otelhttp handler</code> of OpenTelemetry-Go. This flaw means MTV versions before MTV 2.5.3 are vulnerable to a memory leak caused by <code class="literal">http.user_agent</code> and <code class="literal">http.method</code> having unbound cardinality, which could allow a remote, unauthenticated attacker to exhaust the server’s memory by sending many malicious requests, affecting the availability. <a class="link" href="https://issues.redhat.com/browse/MTV-795">(MTV-795)</a>
				</p></div><p>
				This issue has been resolved in MTV 2.5.3. It is advised to update to this version of MTV or later.
			</p><p>
				For more information, see <a class="link" href="https://access.redhat.com/security/cve/CVE-2023-45142">CVE-2023-45142</a>.
			</p><div class="formalpara"><p class="title"><strong>CVE-2023-39322: QUIC connections do not set an upper bound on the amount of data buffered when reading post-handshake messages</strong></p><p>
					A flaw was found in Golang. This flaw means MTV versions before MTV 2.5.3 are vulnerable to QUIC connections not setting an upper bound on the amount of data buffered when reading post-handshake messages, allowing a malicious QUIC connection to cause unbounded memory growth. With the fix, connections now consistently reject messages larger than 65KiB in size. <a class="link" href="https://issues.redhat.com/browse/MTV-708">(MTV-708)</a>
				</p></div><p>
				This issue has been resolved in MTV 2.5.3. It is advised to update to this version of MTV or later.
			</p><p>
				For more information, see <a class="link" href="https://access.redhat.com/security/cve/CVE-2023-39322">CVE-2023-39322</a>.
			</p><div class="formalpara"><p class="title"><strong>CVE-2023-39321: Processing an incomplete post-handshake message for a QUIC connection can cause a panic</strong></p><p>
					A flaw was found in Golang. This flaw means MTV versions before MTV 2.5.3 are vulnerable to processing an incomplete post-handshake message for a QUIC connection, which causes a panic. <a class="link" href="https://issues.redhat.com/browse/MTV-693">(MTV-693)</a>
				</p></div><p>
				This issue has been resolved in MTV 2.5.3. It is advised to update to this version of MTV or later.
			</p><p>
				For more information, see <a class="link" href="https://access.redhat.com/security/cve/CVE-2023-39321">CVE-2023-39321</a>.
			</p><div class="formalpara"><p class="title"><strong>CVE-2023-39319: Flaw in html/template package</strong></p><p>
					A flaw was found in the Golang <code class="literal">html/template</code> package used in MTV. This flaw means MTV versions before MTV 2.5.3 are vulnerable, as the <code class="literal">html/template</code> package did not properly handle occurrences of <code class="literal">&lt;script</code>, <code class="literal">&lt;!--</code>, and <code class="literal">&lt;/script</code> within JavaScript literals in <code class="literal">&lt;script&gt;</code> contexts. This flaw could cause the template parser to improperly consider script contexts to be terminated early, causing actions to be improperly escaped, which could be leveraged to perform an <code class="literal">XSS</code> attack. <a class="link" href="https://issues.redhat.com/browse/MTV-693">(MTV-693)</a>
				</p></div><p>
				This issue has been resolved in MTV 2.5.3. It is advised to update to this version of MTV or later.
			</p><p>
				For more information, see <a class="link" href="https://access.redhat.com/security/cve/CVE-2023-39319">CVE-2023-39319</a>.
			</p><div class="formalpara"><p class="title"><strong>CVE-2023-39318: Flaw in html/template package</strong></p><p>
					A flaw was found in the Golang <code class="literal">html/template</code> package used in MTV. This flaw means MTV versions before MTV 2.5.3 are vulnerable as the <code class="literal">html/template</code> package did not properly handle HMTL-like <code class="literal">""</code> comment tokens, nor hashbang <code class="literal">\#!</code> comment tokens. This flaw could cause the template parser to improperly interpret the contents of <code class="literal">&lt;script&gt;</code> contexts, causing actions to be improperly escaped, which could be leveraged to perform an <code class="literal">XSS</code> attack. <a class="link" href="https://issues.redhat.com/browse/MTV-693">(MTV-693)</a>
				</p></div><p>
				This issue has been resolved in MTV 2.5.3. It is advised to update to this version of MTV or later.
			</p><p>
				For more information, see <a class="link" href="https://access.redhat.com/security/cve/CVE-2023-39318">CVE-2023-39318</a>.
			</p><div class="formalpara"><p class="title"><strong>Logs archive file downloaded from UI includes logs related to deleted migration plan/VM</strong></p><p>
					In earlier releases of MTV 2.6, the log files downloaded from UI could contain logs that are related to an earlier migration plan. <a class="link" href="https://issues.redhat.com/browse/MTV-783">(MTV-783)</a>
				</p></div><p>
				This issue has been resolved in MTV 2.5.3.
			</p><div class="formalpara"><p class="title"><strong>Extending a VM disk in RHV is not reflected in the MTV inventory</strong></p><p>
					In earlier releases of MTV 2.6, the size of disks that are extended in RHV was not adequately monitored. This resulted in the inability to migrate virtual machines with extended disks from a RHV provider. <a class="link" href="https://issues.redhat.com/browse/MTV-830">(MTV-830)</a>
				</p></div><p>
				This issue has been resolved in MTV 2.5.3.
			</p><div class="formalpara"><p class="title"><strong>Filesystem overhead configurable</strong></p><p>
					In earlier releases of MTV 2.6, the filesystem overhead for new persistent volumes was hard-coded to 10%. The overhead was insufficient for certain filesystem types, resulting in failures during cold-migrations from RHV and OSP to the cluster where MTV is deployed. In other filesystem types, the hard-coded overhead was too high, resulting in excessive storage consumption.
				</p></div><p>
				In MTV 2.5.3, the filesystem overhead can be configured, as it is no longer hard-coded. If your migration allocates persistent volumes without CDI, you can adjust the file system overhead. You adjust the file system overhead by adding the following label and value to the <code class="literal">spec</code> portion of the <code class="literal">forklift-controller</code> CR:
			</p><pre class="programlisting language-yaml">spec:
  `controller_filesystem_overhead: &lt;percentage&gt;` <span id="CO1-1"/><span class="callout">1</span></pre><div class="calloutlist"><dl class="calloutlist"><dt><a href="#CO1-1"><span class="callout">1</span></a> </dt><dd><div class="para">
						The percentage of overhead. If this label is not added, the default value of 10% is used. This setting is valid only if the <code class="literal">storageclass</code> is <code class="literal">filesystem</code>. <a class="link" href="https://issues.redhat.com/browse/MTV-699">(MTV-699)</a>
					</div></dd></dl></div><div class="formalpara"><p class="title"><strong>Ensure up-to-date data is displayed in the create and update provider forms</strong></p><p>
					In earlier releases of MTV, the create and update provider forms could have presented stale data.
				</p></div><p>
				This issue is resolved in MTV 2.6, the new forms of create and update provider display up-to-date properties of the provider. <a class="link" href="https://issues.redhat.com/browse/MTV-603">(MTV-603)</a>
			</p><div class="formalpara"><p class="title"><strong>Snapshots that are created during a migration in OpenStack are not deleted</strong></p><p>
					In earlier releases of MTV, the <code class="literal">Migration Controller</code> service did not delete snapshots that were created during a migration of source virtual machines in OpenStack automatically.
				</p></div><p>
				This issue is resolved in MTV 2.6, all the snapshots created during the migration are removed after the migration has been completed. <a class="link" href="https://issues.redhat.com/browse/MTV-620">(MTV-620)</a>
			</p><div class="formalpara"><p class="title"><strong>RHV snapshots are not deleted after a successful migration</strong></p><p>
					In earlier releases of MTV, the <code class="literal">Migration Controller</code> service did not delete snapshots automatically after a successful warm migration of a VM from RHV.
				</p></div><p>
				This issue is resolved in MTV 2.6, the snapshots generated during migration are removed after a successful migration, and the original snapshots are not removed after a successful migration. <a class="link" href="https://issues.redhat.com/browse/MTV-349">(MTV-349)</a>
			</p><div class="formalpara"><p class="title"><strong>Warm migration fails when cutover conflicts with precopy</strong></p><p>
					In earlier releases of MTV, the cutover operation failed when it was triggered while precopy was being performed. The VM was locked in RHV and therefore the <code class="literal">ovirt-engine</code> rejected the snapshot creation, or disk transfer, operation.
				</p></div><p>
				This issue is resolved in MTV 2.6, the cutover operation is triggered, but it is not performed at that time because the VM is locked. Once the precopy operation completes, the cutover operation is triggered. <a class="link" href="https://issues.redhat.com/browse/MTV-686">(MTV-686)</a>
			</p><div class="formalpara"><p class="title"><strong>Warm migration fails when VM is locked</strong></p><p>
					In earlier releases of MTV, triggering a warm migration while there was an ongoing operation in RHV that locked the VM caused the migration to fail because it could not trigger the snapshot creation.
				</p></div><p>
				This issue is resolved in MTV 2.6, warm migration does not fail when an operation that locks the VM is performed in RHV. The migration does not fail, but starts when the VM is unlocked. <a class="link" href="https://issues.redhat.com/browse/MTV-687">(MTV-687)</a>
			</p><div class="formalpara"><p class="title"><strong>Deleting migrated VM does not remove PVC and PV</strong></p><p>
					In earlier releases of MTV, when removing a VM that was migrated, its persistent volume claims (PVCs) and physical volumes (PV) were not deleted.
				</p></div><p>
				This issue is resolved in MTV 2.6, PVCs and PVs are deleted when deleting migrated VM.<a class="link" href="https://issues.redhat.com/browse/MTV-492">(MTV-492)</a>
			</p><div class="formalpara"><p class="title"><strong>PVC deletion hangs after archiving and deleting migration plan</strong></p><p>
					In earlier releases of MTV, when a migration failed, its PVCs and PVs were not deleted as expected when its migration plan was archived and deleted.
				</p></div><p>
				This issue is resolved in MTV 2.6, PVCs are deleted when archiving and deleting migration plan.<a class="link" href="https://issues.redhat.com/browse/MTV-493">(MTV-493)</a>
			</p><div class="formalpara"><p class="title"><strong>VM with multiple disks can boot from a non-bootable disk after migration</strong></p><p>
					In earlier releases of MTV, VM with multiple disks that were migrated might not have been able to boot on the target Red Hat OpenShift cluster.
				</p></div><p>
				This issue is resolved in MTV 2.6, VM with multiple disks that are migrated can boot on the target Red Hat OpenShift cluster. <a class="link" href="https://issues.redhat.com/browse/MTV-433">(MTV-433)</a>
			</p><div class="formalpara"><p class="title"><strong>Transfer network not taken into account for cold migrations from vSphere</strong></p><p>
					In MTV releases 2.4.0-2.5.3, cold migrations from vSphere to the local cluster on which MTV was deployed did not take a specified transfer network into account. This issue is resolved in MTV 2.5.4. <a class="link" href="https://issues.redhat.com/browse/MTV-846">(MTV-846)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Fix migration of VMs with multi-boot guest operating system from vSphere</strong></p><p>
					In MTV 2.5.6, the virt-v2v arguments include <code class="literal">–root first</code>, which mitigates an issue with multi-boot VMs where the pod fails. This is a fix for a regression that was introduced in MTV 2.4, in which the <span class="emphasis"><em>--root</em></span> argument was dropped. <a class="link" href="https://issues.redhat.com/browse/MTV-987">(MTV-987)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Errors logged in populator pods are improved</strong></p><p>
					In earlier releases of MTV 2.6, populator pods were always restarted on failure. This made it difficult to gather the logs from the failed pods. In MTV 2.5.3, the number of restarts of populator pods is limited to three times. On the third and final time, the populator pod remains in the fail status and its logs can then be easily gathered by <code class="literal">must-gather</code> and by <code class="literal">forklift-controller</code> to know this step has failed. <a class="link" href="https://issues.redhat.com/browse/MTV-818">(MTV-818)</a>
				</p></div><div class="formalpara"><p class="title"><strong>npm IP package vulnerability</strong></p><p>
					A vulnerability found in the Node.js Package Manager (npm) IP Package can allow an attacker to obtain sensitive information and obtain access to normally inaccessible resources. <a class="link" href="https://issues.redhat.com/browse/MTV-941">MTV-941</a>
				</p></div><p>
				This issue has been resolved in MTV 2.5.6.
			</p><p>
				For more information, see <a class="link" href="https://access.redhat.com/security/cve/cve-2023-42282">CVE-2023-42282</a>
			</p><div class="formalpara"><p class="title"><strong>Flaw was found in the Golang net/http/internal package</strong></p><p>
					A flaw was found in the versions of the Golang <code class="literal">net/http/internal</code> package, that were used in earlier releases of MTV. This flaw could allow a malicious user to send an HTTP request and cause the receiver to read more bytes from the network than are in the body (up to 1GiB), causing the receiver to fail reading the response, possibly leading to a Denial of Service (DoS). This issue has been resolved in MTV 2.5.6.
				</p></div><p>
				For more information, see <a class="link" href="https://access.redhat.com/security/cve/cve-2023-39326">CVE-2023-39326</a>.
			</p><p>
				For a complete list of all resolved issues in this release, see the list of <a class="link" href="https://issues.redhat.com/issues/?filter=12431371">Resolved Issues</a> in Jira.
			</p></section><section class="section" id="upgrade-notes-25_release-notes"><div class="titlepage"><div><div><h2 class="title">2.5. Upgrade notes</h2></div></div></div><p>
				It is recommended to upgrade from MTV 2.4.2 to MTV 2.6.
			</p><div class="formalpara"><p class="title"><strong>Upgrade from 2.4.0 fails</strong></p><p>
					When upgrading from MTV 2.4.0 to a later version, the operation fails with an error that says the field <span class="emphasis"><em>spec.selector</em></span> of deployment <code class="literal">forklift-controller</code> is immutable. Workaround: Remove the custom resource <code class="literal">forklift-controller</code> of type <code class="literal">ForkliftController</code> from the installed namespace, and recreate it. Refresh the Red Hat OpenShift console once the <code class="literal">forklift-console-plugin</code> pod runs to load the upgraded MTV web console. <a class="link" href="https://issues.redhat.com/browse/MTV-518">(MTV-518)</a>
				</p></div></section></section><section class="chapter" id="rn-24_release-notes"><div class="titlepage"><div><div><h1 class="title">Chapter 3. Migration Toolkit for Virtualization 2.4</h1></div></div></div><p>
			Migrate virtual machines (VMs) from VMware vSphere or Red Hat Virtualization or OpenStack to OpenShift Virtualization with the Migration Toolkit for Virtualization (MTV).
		</p><p>
			The release notes describe technical changes, new features and enhancements, and known issues.
		</p><section class="section" id="technical-changes-24_release-notes"><div class="titlepage"><div><div><h2 class="title">3.1. Technical changes</h2></div></div></div><p>
				This release has the following technical changes:
			</p><div class="formalpara"><p class="title"><strong>Faster disk image migration from RHV</strong></p><p>
					Disk images are not converted anymore using virt-v2v when migrating from RHV. This change speeds up migrations and also allows migration for guest operating systems that are not supported by virt-vsv. <a class="link" href="https://github.com/kubev2v/forklift-controller/issues/403">(forklift-controller#403)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Faster disk transfers by ovirt-imageio client (ovirt-img)</strong></p><p>
					Disk transfers use <code class="literal">ovirt-imageio</code> client (ovirt-img) instead of Containerized Data Import (CDI) when migrating from RHV to the local OpenShift Container Platform cluster, accelerating the migration.
				</p></div><div class="formalpara"><p class="title"><strong>Faster migration using conversion pod disk transfer</strong></p><p>
					When migrating from vSphere to the local OpenShift Container Platform cluster, the conversion pod transfers the disk data instead of Containerized Data Importer (CDI), accelerating the migration.
				</p></div><div class="formalpara"><p class="title"><strong>Migrated virtual machines are not scheduled on the target OCP cluster</strong></p><p>
					The migrated virtual machines are no longer scheduled on the target OpenShift Container Platform cluster. This enables migrating VMs that cannot start due to limit constraints on the target at migration time.
				</p></div><div class="formalpara"><p class="title"><strong>StorageProfile resource needs to be updated for a non-provisioner storage class</strong></p><p>
					You must update the <code class="literal">StorageProfile</code> resource with <code class="literal">accessModes</code> and <code class="literal">volumeMode</code> for non-provisioner storage classes such as NFS.
				</p></div><div class="formalpara"><p class="title"><strong>VDDK 8 can be used in the VDDK image</strong></p><p>
					Previous versions of MTV supported only using VDDK version 7 for the VDDK image. MTV supports both versions 7 and 8, as follows:
				</p></div><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						If you are migrating to OCP 4.12 or earlier, use VDDK version 7.
					</li><li class="listitem">
						If you are migrating to OCP 4.13 or later, use VDDK version 8.
					</li></ul></div></section><section class="section" id="new-features-and-enhancements-24_release-notes"><div class="titlepage"><div><div><h2 class="title">3.2. New features and enhancements</h2></div></div></div><p>
				This release has the following features and improvements:
			</p><div class="formalpara"><p class="title"><strong>OpenStack migration</strong></p><p>
					MTV now supports migrations with OpenStack as a source provider. This feature is a provided as a Technology Preview and only supports cold migrations.
				</p></div><div class="formalpara"><p class="title"><strong>OCP console plugin</strong></p><p>
					The Migration Toolkit for Virtualization Operator now integrates the MTV web console into the Red Hat OpenShift web console. The new UI operates as an OCP Console plugin that adds the sub-menu <code class="literal">Migration</code> to the navigation bar. It is implemented in version 2.4, disabling the old UI. You can enable the old UI by setting <code class="literal">feature_ui: true</code> in ForkliftController. <a class="link" href="https://issues.redhat.com/browse/MTV-427">(MTV-427)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Skip certification option</strong></p><p>
					<span class="emphasis"><em>Skip certificate validation</em></span> option was added to VMware and RHV providers. If selected, the provider’s certificate will not be validated and the UI will not ask for specifying a CA certificate.
				</p></div><div class="formalpara"><p class="title"><strong>Only third-party certificate required</strong></p><p>
					Only the third-party certificate needs to be specified when defining a RHV provider that sets with the Manager CA certificate.
				</p></div><div class="formalpara"><p class="title"><strong>Conversion of VMs with RHEL9 guest operating system</strong></p><p>
					Cold migrations from vSphere to a local Red Hat OpenShift cluster use virt-v2v on RHEL 9. <a class="link" href="https://issues.redhat.com/browse/MTV-332">(MTV-332)</a>
				</p></div></section><section class="section" id="known-issues-24_release-notes"><div class="titlepage"><div><div><h2 class="title">3.3. Known issues</h2></div></div></div><p>
				This release has the following known issues:
			</p><div class="formalpara"><p class="title"><strong>Deleting migration plan does not remove temporary resources</strong></p><p>
					Deleting a migration plan does not remove temporary resources such as importer pods, conversion pods, config maps, secrets, failed VMs and data volumes. You must archive a migration plan before deleting it to clean up the temporary resources. <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2018974">(BZ#2018974)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Unclear error status message for VM with no operating system</strong></p><p>
					The error status message for a VM with no operating system on the <span class="strong strong"><strong>Plans</strong></span> page of the web console does not describe the reason for the failure. <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2008846">(BZ#22008846)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Log archive file includes logs of a deleted migration plan or VM</strong></p><p>
					If deleting a migration plan and then running a new migration plan with the same name, or if deleting a migrated VM and then remigrate the source VM, then the log archive file created by the MTV web console might include the logs of the deleted migration plan or VM. <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2023764">(BZ#2023764)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Migration of virtual machines with encrypted partitions fails during conversion</strong></p><p>
					vSphere only: Migrations from RHV and OpenStack don’t fail, but the encryption key may be missing on the target OCP cluster.
				</p></div><div class="formalpara"><p class="title"><strong>Snapshots that are created during the migration in OpenStack are not deleted</strong></p><p>
					The Migration Controller service does not delete snapshots that are created during the migration for source virtual machines in OpenStack automatically. Workaround: the snapshots can be removed manually on OpenStack.
				</p></div><div class="formalpara"><p class="title"><strong>RHV snapshots are not deleted after a successful migration</strong></p><p>
					The Migration Controller service does not delete snapshots automatically after a successful warm migration of a RHV VM. Workaround: Snapshots can be removed from RHV instead. <a class="link" href="https://issues.redhat.com/browse/MTV-349">(MTV-349)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Migration fails during precopy/cutover while a snapshot operation is executed on the source VM</strong></p><p>
					Some warm migrations from RHV might fail. When running a migration plan for warm migration of multiple VMs from RHV, the migrations of some VMs might fail during the cutover stage. In that case, restart the migration plan and set the cutover time for the VM migrations that failed in the first run.
				</p></div><p>
				Warm migration from RHV fails if a snapshot operation is performed on the source VM. If the user performs a snapshot operation on the source VM at the time when a migration snapshot is scheduled, the migration fails instead of waiting for the user’s snapshot operation to finish. <a class="link" href="https://issues.redhat.com/browse/MTV-456">(MTV-456)</a>
			</p><div class="formalpara"><p class="title"><strong>Cannot schedule migrated VM with multiple disks to more than one storage classes of type hostPath</strong></p><p>
					When migrating a VM with multiple disks to more than one storage classes of type hostPath, it may result in a VM that cannot be scheduled. Workaround: It is recommended to use shared storage on the target OCP cluster.
				</p></div><div class="formalpara"><p class="title"><strong>Deleting migrated VM does not remove PVC and PV</strong></p><p>
					When removing a VM that was migrated, its persistent volume claims (PVCs) and physical volumes (PV) are not deleted. Workaround: remove the CDI importer pods and then remove the remaining PVCs and PVs. <a class="link" href="https://issues.redhat.com/browse/MTV-492">(MTV-492)</a>
				</p></div><div class="formalpara"><p class="title"><strong>PVC deletion hangs after archiving and deleting migration plan</strong></p><p>
					When a migration fails, its PVCs and PVs are not deleted as expected when its migration plan is archived and deleted. Workaround: Remove the CDI importer pods and then remove the remaining PVCs and PVs. <a class="link" href="https://issues.redhat.com/browse/MTV-493">(MTV-493)</a>
				</p></div><div class="formalpara"><p class="title"><strong>VM with multiple disks may boot from non-bootable disk after migration</strong></p><p>
					VM with multiple disks that was migrated might not be able to boot on the target OCP cluster. Workaround: Set the boot order appropriately to boot from the bootable disk. <a class="link" href="https://issues.redhat.com/browse/MTV-433">(MTV-433)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Non-supported guest operating systems in warm migrations</strong></p><p>
					Warm migrations and migrations to remote OCP clusters from vSphere do not support all types of guest operating systems that are supported in cold migrations to the local OCP cluster. It is a consequence of using RHEL 8 in the former case and RHEL 9 in the latter case.<br/> See <a class="link" href="https://access.redhat.com/articles/1351473">Converting virtual machines from other hypervisors to KVM with virt-v2v in RHEL 7, RHEL 8, and RHEL 9</a> for the list of supported guest operating systems.
				</p></div><div class="formalpara"><p class="title"><strong>VMs from vSphere with RHEL 9 guest operating system may start with network interfaces that are down</strong></p><p>
					When migrating VMs that are installed with RHEL 9 as guest operating system from vSphere, their network interfaces could be disabled when they start in OpenShift Virtualization. <a class="link" href="https://issues.redhat.com/browse/MTV-491">(MTV-491)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Upgrade from 2.4.0 fails</strong></p><p>
					When upgrading from MTV 2.4.0 to a later version, the operation fails with an error that says the field <span class="emphasis"><em>spec.selector</em></span> of deployment <code class="literal">forklift-controller</code> is immutable. Workaround: remove the custom resource <code class="literal">forklift-controller</code> of type <code class="literal">ForkliftController</code> from the installed namespace, and recreate it. The user needs to refresh the OCP Console once the <code class="literal">forklift-console-plugin</code> pod runs to load the upgraded MTV web console. <a class="link" href="https://issues.redhat.com/browse/MTV-518">(MTV-518)</a>
				</p></div></section><section class="section" id="resolved-issues-24_release-notes"><div class="titlepage"><div><div><h2 class="title">3.4. Resolved issues</h2></div></div></div><p>
				This release has the following resolved issues:
			</p><div class="formalpara"><p class="title"><strong>Multiple HTTP/2 enabled web servers are vulnerable to a DDoS attack (Rapid Reset Attack)</strong></p><p>
					A flaw was found in handling multiplexed streams in the HTTP/2 protocol. In previous releases of MTV, the HTTP/2 protocol allowed a denial of service (server resource consumption) because request cancellation could reset multiple streams quickly. The server had to set up and tear down the streams while not hitting any server-side limit for the maximum number of active streams per connection, which resulted in a denial of service due to server resource consumption.
				</p></div><p>
				This issue has been resolved in MTV 2.4.3 and 2.5.2. It is advised to update to one of these versions of MTV or later.
			</p><p>
				For more information, see <a class="link" href="https://access.redhat.com/security/cve/cve-2023-44487">CVE-2023-44487 (Rapid Reset Attack)</a> and <a class="link" href="https://access.redhat.com/security/cve/cve-2023-39325">CVE-2023-39325 (Rapid Reset Attack)</a>.
			</p><div class="formalpara"><p class="title"><strong>Improve invalid/conflicting VM name handling</strong></p><p>
					Improve the automatic renaming of VMs during migration to fit RFC 1123. This feature that was introduced in 2.3.4 is enhanced to cover more special cases. <a class="link" href="https://issues.redhat.com/browse/MTV-212">(MTV-212)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Prevent locking user accounts due to incorrect credentials</strong></p><p>
					If a user specifies an incorrect password for RHV providers, they are no longer locked in RHV. An error returns when the RHV manager is accessible and adding the provider. If the RHV manager is inaccessible, the provider is added, but there would be no further attempt after failing, due to incorrect credentials. <a class="link" href="https://issues.redhat.com/browse/MTV-324">(MTV-324)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Users without cluster-admin role can create new providers</strong></p><p>
					Previously, the <code class="literal">cluster-admin</code> role was required to browse and create providers. In this release, users with sufficient permissions on MTV resources (providers, plans, migrations, NetworkMaps, StorageMaps, hooks) can operate MTV without cluster-admin permissions. <a class="link" href="https://issues.redhat.com/browse/MTV-334">(MTV-334)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Convert i440fx to q35</strong></p><p>
					Migration of virtual machines with i440fx chipset is now supported. The chipset is converted to q35 during the migration. <a class="link" href="https://issues.redhat.com/browse/MTV-430">(MTV-430)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Preserve the UUID setting in SMBIOS for a VM that is migrated from RHV</strong></p><p>
					The Universal Unique ID (UUID) number within the System Management BIOS (SMBIOS) no longer changes for VMs that are migrated from RHV. This enhancement enables applications that operate within the guest operating system and rely on this setting, such as for licensing purposes, to operate on the target OCP cluster in a manner similar to that of RHV. <a class="link" href="https://issues.redhat.com/browse/MTV-597">(MTV-597)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Do not expose password for RHV in error messages</strong></p><p>
					Previously, the password that was specified for RHV manager appeared in error messages that were displayed in the web console and logs when failing to connect to RHV. In this release, error messages that are generated when failing to connect to RHV do not reveal the password for RHV manager.
				</p></div><div class="formalpara"><p class="title"><strong>QEMU guest agent is now installed on migrated VMs</strong></p><p>
					The QEMU guest agent is installed on VMs during cold migration from vSphere. <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2018062">(BZ#2018062)</a>
				</p></div></section></section><section class="chapter" id="rn-23_release-notes"><div class="titlepage"><div><div><h1 class="title">Chapter 4. Migration Toolkit for Virtualization 2.3</h1></div></div></div><p>
			You can migrate virtual machines (VMs) from VMware vSphere or Red Hat Virtualization to OpenShift Virtualization with the Migration Toolkit for Virtualization (MTV).
		</p><p>
			The release notes describe technical changes, new features and enhancements, and known issues.
		</p><section class="section" id="technical-changes-23_release-notes"><div class="titlepage"><div><div><h2 class="title">4.1. Technical changes</h2></div></div></div><p>
				This release has the following technical changes:
			</p><div class="formalpara"><p class="title"><strong>Setting the VddkInitImage path is part of the procedure of adding VMware provider.</strong></p><p>
					In the web console, you enter the VddkInitImage path when adding a VMware provider. Alternatively, from the CLI, you add the VddkInitImage path to the <code class="literal">Provider</code> CR for VMware migrations.
				</p></div><div class="formalpara"><p class="title"><strong>The StorageProfile resource needs to be updated for a non-provisioner storage class</strong></p><p>
					You must update the <code class="literal">StorageProfile</code> resource with <code class="literal">accessModes</code> and <code class="literal">volumeMode</code> for non-provisioner storage classes such as NFS. The documentation includes a link to the relevant procedure.
				</p></div></section><section class="section" id="new-features-and-enhancements-23_release-notes"><div class="titlepage"><div><div><h2 class="title">4.2. New features and enhancements</h2></div></div></div><p>
				This release has the following features and improvements:
			</p><div class="formalpara"><p class="title"><strong>MTV 2.6 supports warm migration from RHV</strong></p><p>
					You can use warm migration to migrate VMs from both VMware and RHV.
				</p></div><div class="formalpara"><p class="title"><strong>The minimal sufficient set of privileges for VMware users is established</strong></p><p>
					VMware users do not have to have full <code class="literal">cluster-admin</code> privileges to perform a VM migration. The minimal sufficient set of user’s privileges is established and documented.
				</p></div><div class="formalpara"><p class="title"><strong>MTV documentation is updated with instructions on using hooks</strong></p><p>
					MTV documentation includes instructions on adding hooks to migration plans and running hooks on VMs.
				</p></div></section><section class="section" id="known-issues-23_release-notes"><div class="titlepage"><div><div><h2 class="title">4.3. Known issues</h2></div></div></div><p>
				This release has the following known issues:
			</p><div class="formalpara"><p class="title"><strong>Some warm migrations from RHV might fail</strong></p><p>
					When you run a migration plan for warm migration of multiple VMs from RHV, the migrations of some VMs might fail during the cutover stage. In that case, restart the migration plan and set the cutover time for the VM migrations that failed in the first run. <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2063531">(<span class="strong strong"><strong>BZ#2063531</strong></span>)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Snapshots are not deleted after warm migration</strong></p><p>
					The Migration Controller service does not delete snapshots automatically after a successful warm migration of a RHV VM. You can <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.9/html-single/virtualization/index#virt-deleting-vm-snapshot-web_virt-managing-vm-snapshots">delete the snapshots manually</a>. <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2053183">(<span class="strong strong"><strong>BZ#22053183</strong></span>)</a>
				</p></div><div class="formalpara"><p class="title"><strong>Warm migration from RHV fails if a snapshot operation is performed on the source VM</strong></p><p>
					If the user performs a snapshot operation on the source VM at the time when a migration snapshot is scheduled, the migration fails instead of waiting for the user’s snapshot operation to finish. <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2057459">(<span class="strong strong"><strong>BZ#2057459</strong></span>)</a>
				</p></div><div class="formalpara"><p class="title"><strong>QEMU guest agent is not installed on migrated VMs</strong></p><p>
					The QEMU guest agent is not installed on migrated VMs. Workaround: Install the QEMU guest agent with a post-migration hook. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2018062"><span class="strong strong"><strong>BZ#2018062</strong></span></a>)
				</p></div><div class="formalpara"><p class="title"><strong>Deleting migration plan does not remove temporary resources.</strong></p><p>
					Deleting a migration plan does not remove temporary resources such as <code class="literal">importer</code> pods, <code class="literal">conversion</code> pods, config maps, secrets, failed VMs and data volumes. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2018974"><span class="strong strong"><strong>BZ#2018974</strong></span></a>) You must archive a migration plan before deleting it in order to clean up the temporary resources.
				</p></div><div class="formalpara"><p class="title"><strong>Unclear error status message for VM with no operating system</strong></p><p>
					The error status message for a VM with no operating system on the <span class="strong strong"><strong>Migration plan details</strong></span> page of the web console does not describe the reason for the failure. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2008846"><span class="strong strong"><strong>BZ#2008846</strong></span></a>)
				</p></div><div class="formalpara"><p class="title"><strong>Log archive file includes logs of a deleted migration plan or VM</strong></p><p>
					If you delete a migration plan and then run a new migration plan with the same name or if you delete a migrated VM and then remigrate the source VM, the log archive file created by the MTV web console might include the logs of the deleted migration plan or VM. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2023764"><span class="strong strong"><strong>BZ#2023764</strong></span></a>)
				</p></div><div class="formalpara"><p class="title"><strong>Migration of virtual machines with encrypted partitions fails during conversion</strong></p><p>
					The problem occurs for both vSphere and RHV migrations.
				</p></div><div class="formalpara"><p class="title"><strong>MTV 2.3.4 only: When the source provider is RHV, duplicating a migration plan fails in either the network mapping stage or the storage mapping stage.</strong></p><p>
					Possible workaround: Delete cache in the browser or restart the browser. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2143191"><span class="strong strong"><strong>BZ#2143191</strong></span></a>)
				</p></div></section></section><section class="chapter" id="rn-22_release-notes"><div class="titlepage"><div><div><h1 class="title">Chapter 5. Migration Toolkit for Virtualization 2.2</h1></div></div></div><p>
			You can migrate virtual machines (VMs) from VMware vSphere or Red Hat Virtualization to OpenShift Virtualization with the Migration Toolkit for Virtualization (MTV).
		</p><p>
			The release notes describe technical changes, new features and enhancements, and known issues.
		</p><section class="section" id="technical-changes-22_release-notes"><div class="titlepage"><div><div><h2 class="title">5.1. Technical changes</h2></div></div></div><p>
				This release has the following technical changes:
			</p><div class="formalpara"><p class="title"><strong>Setting the precopy time interval for warm migration</strong></p><p>
					You can set the time interval between snapshots taken during the precopy stage of warm migration.
				</p></div></section><section class="section" id="new-features-and-enhancements-22_release-notes"><div class="titlepage"><div><div><h2 class="title">5.2. New features and enhancements</h2></div></div></div><p>
				This release has the following features and improvements:
			</p><div class="formalpara"><p class="title"><strong>Creating validation rules</strong></p><p>
					You can create custom validation rules to check the suitability of VMs for migration. Validation rules are based on the VM attributes collected by the <code class="literal">Provider Inventory</code> service and written in <a class="link" href="https://www.openpolicyagent.org/docs/latest/policy-language/">Rego</a>, the Open Policy Agent native query language.
				</p></div><div class="formalpara"><p class="title"><strong>Downloading logs by using the web console</strong></p><p>
					You can download logs for a migration plan or a migrated VM by using the MTV web console.
				</p></div><div class="formalpara"><p class="title"><strong>Duplicating a migration plan by using the web console</strong></p><p>
					You can duplicate a migration plan by using the web console, including its VMs, mappings, and hooks, in order to edit the copy and run as a new migration plan.
				</p></div><div class="formalpara"><p class="title"><strong>Archiving a migration plan by using the web console</strong></p><p>
					You can archive a migration plan by using the MTV web console. Archived plans can be viewed or duplicated. They cannot be run, edited, or unarchived.
				</p></div></section><section class="section" id="known-issues-22_release-notes"><div class="titlepage"><div><div><h2 class="title">5.3. Known issues</h2></div></div></div><p>
				This release has the following known issues:
			</p><div class="formalpara"><p class="title"><strong>Certain Validation service issues do not block migration</strong></p><p>
					Certain <code class="literal">Validation</code> service issues, which are marked as <code class="literal">Critical</code> and display the assessment text, <code class="literal">The VM will not be migrated</code>, do not block migration. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2025977"><span class="strong strong"><strong>BZ#2025977</strong></span></a>)
				</p></div><p>
				The following <code class="literal">Validation</code> service assessments do not block migration:
			</p><div class="table" id="idm46144043724576"><p class="title"><strong>Table 5.1. Issues that do not block migration</strong></p><div class="table-contents"><table class="lt-4-cols lt-7-rows"><colgroup><col style="width: 67%; " class="col_1"/><col style="width: 33%; " class="col_2"/></colgroup><thead><tr><th align="left" valign="top" id="idm46144043751968" scope="col">Assessment</th><th align="left" valign="top" id="idm46144043750880" scope="col">Result</th></tr></thead><tbody><tr><td align="left" valign="top" headers="idm46144043751968">
							<p>
								The disk interface type is not supported by OpenShift Virtualization (only sata, virtio_scsi and virtio interface types are currently supported).
							</p>
							</td><td align="left" valign="top" headers="idm46144043750880">
							<p>
								The migrated VM will have a virtio disk if the source interface is not recognized.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46144043751968">
							<p>
								The NIC interface type is not supported by OpenShift Virtualization (only e1000, rtl8139 and virtio interface types are currently supported).
							</p>
							</td><td align="left" valign="top" headers="idm46144043750880">
							<p>
								The migrated VM will have a virtio NIC if the source interface is not recognized.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46144043751968">
							<p>
								The VM is using a vNIC profile configured for host device passthrough, which is not currently supported by OpenShift Virtualization.
							</p>
							</td><td align="left" valign="top" headers="idm46144043750880">
							<p>
								The migrated VM will have an SR-IOV NIC. The destination network must be set up correctly.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46144043751968">
							<p>
								One or more of the VM’s disks has an illegal or locked status condition.
							</p>
							</td><td align="left" valign="top" headers="idm46144043750880">
							<p>
								The migration will proceed but the disk transfer is likely to fail.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46144043751968">
							<p>
								The VM has a disk with a storage type other than <code class="literal">image</code>, and this is not currently supported by OpenShift Virtualization.
							</p>
							</td><td align="left" valign="top" headers="idm46144043750880">
							<p>
								The migration will proceed but the disk transfer is likely to fail.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46144043751968">
							<p>
								The VM has one or more snapshots with disks in ILLEGAL state. This is not currently supported by OpenShift Virtualization.
							</p>
							</td><td align="left" valign="top" headers="idm46144043750880">
							<p>
								The migration will proceed but the disk transfer is likely to fail.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46144043751968">
							<p>
								The VM has USB support enabled, but USB devices are not currently supported by OpenShift Virtualization.
							</p>
							</td><td align="left" valign="top" headers="idm46144043750880">
							<p>
								The migrated VM will not have USB devices.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46144043751968">
							<p>
								The VM is configured with a watchdog device, which is not currently supported by OpenShift Virtualization.
							</p>
							</td><td align="left" valign="top" headers="idm46144043750880">
							<p>
								The migrated VM will not have a watchdog device.
							</p>
							</td></tr><tr><td align="left" valign="top" headers="idm46144043751968">
							<p>
								The VM’s status is not <code class="literal">up</code> or <code class="literal">down</code>.
							</p>
							</td><td align="left" valign="top" headers="idm46144043750880">
							<p>
								The migration will proceed but it might hang if the VM cannot be powered off.
							</p>
							</td></tr></tbody></table></div></div><div class="formalpara"><p class="title"><strong>QEMU guest agent is not installed on migrated VMs</strong></p><p>
					The QEMU guest agent is not installed on migrated VMs. Workaround: Install the QEMU guest agent with a post-migration hook. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2018062"><span class="strong strong"><strong>BZ#2018062</strong></span></a>)
				</p></div><div class="formalpara"><p class="title"><strong>Missing resource causes error message in current.log file</strong></p><p>
					If a resource does not exist, for example, if the <code class="literal">virt-launcher</code> pod does not exist because the migrated VM is powered off, its log is unavailable.
				</p></div><p>
				The following error appears in the missing resource’s <code class="literal">current.log</code> file when it is downloaded from the web console or created with the <code class="literal">must-gather</code> tool: <code class="literal">error: expected 'logs [-f] [-p] (POD | TYPE/NAME) [-c CONTAINER]'.</code> (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2023260"><span class="strong strong"><strong>BZ#2023260</strong></span></a>)
			</p><div class="formalpara"><p class="title"><strong>Importer pod log is unavailable after warm migration</strong></p><p>
					Retaining the <code class="literal">importer</code> pod for debug purposes causes warm migration to hang during the precopy stage. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2016290"><span class="strong strong"><strong>BZ#2016290</strong></span></a>)
				</p></div><p>
				As a temporary workaround, the <code class="literal">importer</code> pod is removed at the end of the precopy stage so that the precopy succeeds. However, this means that the <code class="literal">importer</code> pod log is not retained after warm migration is complete. You can only view the <code class="literal">importer</code> pod log by using the <code class="literal">oc logs -f &lt;cdi-importer_pod&gt;</code> command during the precopy stage.
			</p><p>
				This issue only affects the <code class="literal">importer</code> pod log and warm migration. Cold migration and the <code class="literal">virt-v2v</code> logs are not affected.
			</p><div class="formalpara"><p class="title"><strong>Deleting migration plan does not remove temporary resources.</strong></p><p>
					Deleting a migration plan does not remove temporary resources such as <code class="literal">importer</code> pods, <code class="literal">conversion</code> pods, config maps, secrets, failed VMs and data volumes. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2018974"><span class="strong strong"><strong>BZ#2018974</strong></span></a>) You must archive a migration plan before deleting it in order to clean up the temporary resources.
				</p></div><div class="formalpara"><p class="title"><strong>Unclear error status message for VM with no operating system</strong></p><p>
					The error status message for a VM with no operating system on the <span class="strong strong"><strong>Migration plan details</strong></span> page of the web console does not describe the reason for the failure. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2008846"><span class="strong strong"><strong>BZ#2008846</strong></span></a>)
				</p></div><div class="formalpara"><p class="title"><strong>Network, storage, and VM referenced by name in the <code class="literal">Plan</code> CR are not displayed in the web console.</strong></p><p>
					If a Plan CR references storage, network, or VMs by name instead of by ID, the resources do not appear in the MTV web console. The migration plan cannot be edited or duplicated. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1986020"><span class="strong strong"><strong>BZ#1986020</strong></span></a>)
				</p></div><div class="formalpara"><p class="title"><strong>Log archive file includes logs of a deleted migration plan or VM</strong></p><p>
					If you delete a migration plan and then run a new migration plan with the same name or if you delete a migrated VM and then remigrate the source VM, the log archive file created by the MTV web console might include the logs of the deleted migration plan or VM. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2023764"><span class="strong strong"><strong>BZ#2023764</strong></span></a>)
				</p></div><div class="formalpara"><p class="title"><strong>If a target VM is deleted during migration, its migration status is <code class="literal">Succeeded</code> in the <code class="literal">Plan</code> CR</strong></p><p>
					If you delete a target <code class="literal">VirtualMachine</code> CR during the <span class="emphasis"><em>Convert image to kubevirt</em></span> step of the migration, the <span class="strong strong"><strong>Migration details</strong></span> page of the web console displays the state of the step as <code class="literal">VirtualMachine CR not found</code>. However, the status of the VM migration is <code class="literal">Succeeded</code> in the <code class="literal">Plan</code> CR file and in the web console. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2031529"><span class="strong strong"><strong>BZ#2031529</strong></span></a>)
				</p></div></section></section><section class="chapter" id="rn-21_release-notes"><div class="titlepage"><div><div><h1 class="title">Chapter 6. Migration Toolkit for Virtualization 2.1</h1></div></div></div><p>
			You can migrate virtual machines (VMs) from VMware vSphere or Red Hat Virtualization to OpenShift Virtualization with the Migration Toolkit for Virtualization (MTV).
		</p><p>
			The release notes describe new features and enhancements, known issues, and technical changes.
		</p><section class="section" id="technical-changes-21_release-notes"><div class="titlepage"><div><div><h2 class="title">6.1. Technical changes</h2></div></div></div><div class="formalpara"><p class="title"><strong>VDDK image added to <code class="literal">HyperConverged</code> custom resource</strong></p><p>
					The VMware Virtual Disk Development Kit (VDDK) SDK image must be added to the <code class="literal">HyperConverged</code> custom resource. Before this release, it was referenced in the <code class="literal">v2v-vmware</code> config map.
				</p></div></section><section class="section" id="new-features-and-enhancements-21_release-notes"><div class="titlepage"><div><div><h2 class="title">6.2. New features and enhancements</h2></div></div></div><p>
				This release adds the following features and improvements.
			</p><div class="formalpara"><p class="title"><strong>Cold migration from Red Hat Virtualization</strong></p><p>
					You can perform a cold migration of VMs from Red Hat Virtualization.
				</p></div><div class="formalpara"><p class="title"><strong>Migration hooks</strong></p><p>
					You can create migration hooks to run Ansible playbooks or custom code before or after migration.
				</p></div><div class="formalpara"><p class="title"><strong>Filtered <code class="literal">must-gather</code> data collection</strong></p><p>
					You can specify options for the <code class="literal">must-gather</code> tool that enable you to filter the data by namespace, migration plan, or VMs.
				</p></div><div class="formalpara"><p class="title"><strong>SR-IOV network support</strong></p><p>
					You can migrate VMs with a single root I/O virtualization (SR-IOV) network interface if the OpenShift Virtualization environment has an SR-IOV network.
				</p></div></section><section class="section" id="known-issues-21_release-notes"><div class="titlepage"><div><div><h2 class="title">6.3. Known issues</h2></div></div></div><div class="formalpara"><p class="title"><strong>QEMU guest agent is not installed on migrated VMs</strong></p><p>
					The QEMU guest agent is not installed on migrated VMs. Workaround: Install the QEMU guest agent with a post-migration hook. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2018062"><span class="strong strong"><strong>BZ#2018062</strong></span></a>)
				</p></div><div class="formalpara"><p class="title"><strong>Disk copy stage does not progress</strong></p><p>
					The disk copy stage of a RHV VM does not progress and the MTV web console does not display an error message. <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1990596">(<span class="strong strong"><strong>BZ#1990596</strong></span>)</a>
				</p></div><p>
				The cause of this problem might be one of the following conditions:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						The storage class does not exist on the target cluster.
					</li><li class="listitem">
						The VDDK image has not been added to the <code class="literal">HyperConverged</code> custom resource.
					</li><li class="listitem">
						The VM does not have a disk.
					</li><li class="listitem">
						The VM disk is locked.
					</li><li class="listitem">
						The VM time zone is not set to UTC.
					</li><li class="listitem">
						The VM is configured for a USB device.
					</li></ul></div><p>
				To disable USB devices, see <a class="link" href="https://access.redhat.com/documentation/en-us/red_hat_virtualization/4.4/html-single/virtual_machine_management_guide/index#sect-Configuring_USB_Devices">Configuring USB Devices</a> in the Red Hat Virtualization documentation.
			</p><p>
				To determine the cause:
			</p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem">
						Click <span class="strong strong"><strong>Workloads</strong></span> → <span class="strong strong"><strong>Virtualization</strong></span> in the Red Hat OpenShift web console.
					</li><li class="listitem">
						Click the <span class="strong strong"><strong>Virtual Machines</strong></span> tab.
					</li><li class="listitem">
						Select a virtual machine to open the <span class="strong strong"><strong>Virtual Machine Overview</strong></span> screen.
					</li><li class="listitem">
						Click <span class="strong strong"><strong>Status</strong></span> to view the status of the virtual machine.
					</li></ol></div><div class="formalpara"><p class="title"><strong>VM time zone must be UTC with no offset</strong></p><p>
					The time zone of the source VMs must be UTC with no offset. You can set the time zone to <code class="literal">GMT Standard Time</code> after first assessing the potential impact on the workload. <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1993259">(<span class="strong strong"><strong>BZ#1993259</strong></span>)</a>
				</p></div><div class="formalpara"><p class="title"><strong>RHV resource UUID causes a "Provider not found" error</strong></p><p>
					If a RHV resource UUID is used in a <code class="literal">Host</code>, <code class="literal">NetworkMap</code>, <code class="literal">StorageMap</code>, or <code class="literal">Plan</code> custom resource (CR), a "Provider not found" error is displayed.
				</p></div><p>
				You must use the resource name. <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1994037">(<span class="strong strong"><strong>BZ#1994037</strong></span>)</a>
			</p><div class="formalpara"><p class="title"><strong>Same RHV resource name in different data centers causes ambiguous reference</strong></p><p>
					If a RHV resource name is used in a <code class="literal">NetworkMap</code>, <code class="literal">StorageMap</code>, or <code class="literal">Plan</code> custom resource (CR) and if the same resource name exists in another data center, the <code class="literal">Plan</code> CR displays a critical "Ambiguous reference" condition. You must rename the resource or use the resource UUID in the CR.
				</p></div><p>
				In the web console, the resource name appears twice in the same list without a data center reference to distinguish them. You must rename the resource. <a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1993089">(<span class="strong strong"><strong>BZ#1993089</strong></span>)</a>
			</p><div class="formalpara"><p class="title"><strong>Snapshots are not deleted after warm migration</strong></p><p>
					Snapshots are not deleted automatically after a successful warm migration of a VMware VM. You must delete the snapshots manually in VMware vSphere. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2001270"><span class="strong strong"><strong>BZ#2001270</strong></span></a>)
				</p></div></section></section><section class="chapter" id="rn-20_release-notes"><div class="titlepage"><div><div><h1 class="title">Chapter 7. Migration Toolkit for Virtualization 2.0</h1></div></div></div><p>
			You can migrate virtual machines (VMs) from VMware vSphere with the Migration Toolkit for Virtualization (MTV).
		</p><p>
			The release notes describe new features and enhancements, known issues, and technical changes.
		</p><section class="section" id="new-features-and-enhancements-20_release-notes"><div class="titlepage"><div><div><h2 class="title">7.1. New features and enhancements</h2></div></div></div><p>
				This release adds the following features and improvements.
			</p><div class="formalpara"><p class="title"><strong>Warm migration</strong></p><p>
					Warm migration reduces downtime by copying most of the VM data during a precopy stage while the VMs are running. During the cutover stage, the VMs are stopped and the rest of the data is copied.
				</p></div><div class="formalpara"><p class="title"><strong>Cancel migration</strong></p><p>
					You can cancel an entire migration plan or individual VMs while a migration is in progress. A canceled migration plan can be restarted in order to migrate the remaining VMs.
				</p></div><div class="formalpara"><p class="title"><strong>Migration network</strong></p><p>
					You can select a migration network for the source and target providers for improved performance. By default, data is copied using the VMware administration network and the Red Hat OpenShift pod network.
				</p></div><div class="formalpara"><p class="title"><strong>Validation service</strong></p><p>
					The validation service checks source VMs for issues that might affect migration and flags the VMs with concerns in the migration plan.
				</p></div><div class="admonition important"><div class="admonition_header">Important</div><div><p>
					The validation service is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.
				</p><p>
					For more information about the support scope of Red Hat Technology Preview features, see <a class="link" href="https://access.redhat.com/support/offerings/techpreview/">https://access.redhat.com/support/offerings/techpreview/</a>.
				</p></div></div></section><section class="section" id="known-issues-20_release-notes"><div class="titlepage"><div><div><h2 class="title">7.2. Known issues</h2></div></div></div><p>
				This section describes known issues and mitigations.
			</p><div class="formalpara"><p class="title"><strong>QEMU guest agent is not installed on migrated VMs</strong></p><p>
					The QEMU guest agent is not installed on migrated VMs. Workaround: Install the QEMU guest agent with a post-migration hook. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=2018062"><span class="strong strong"><strong>BZ#2018062</strong></span></a>)
				</p></div><div class="formalpara"><p class="title"><strong>Network map displays a "Destination network not found" error</strong></p><p>
					If the network map remains in a <code class="literal">NotReady</code> state and the <code class="literal">NetworkMap</code> manifest displays a <code class="literal">Destination network not found</code> error, the cause is a missing network attachment definition. You must create a <a class="link" href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html/virtualization/virtual-machines#virt-creating-network-attachment-definition">network attachment definition</a> for each additional destination network before you create the network map. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1971259"><span class="strong strong"><strong>BZ#1971259</strong></span></a>)
				</p></div><div class="formalpara"><p class="title"><strong>Warm migration gets stuck during third precopy</strong></p><p>
					Warm migration uses changed block tracking snapshots to copy data during the precopy stage. The snapshots are created at one-hour intervals by default. When a snapshot is created, its contents are copied to the destination cluster. However, when the third snapshot is created, the first snapshot is deleted and the block tracking is lost. (<a class="link" href="https://bugzilla.redhat.com/show_bug.cgi?id=1969894"><span class="strong strong"><strong>BZ#1969894</strong></span></a>)
				</p></div><p>
				You can do one of the following to mitigate this issue:
			</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">
						Start the cutover stage no more than one hour after the precopy stage begins so that only one internal snapshot is created.
					</li><li class="listitem"><p class="simpara">
						Increase the snapshot interval in the <code class="literal">vm-import-controller-config</code> config map to <code class="literal">720</code> minutes:
					</p><pre class="programlisting language-terminal">$ oc patch configmap/vm-import-controller-config \
  -n openshift-cnv -p '{"data": \
  {"warmImport.intervalMinutes": "720"}}'</pre></li></ul></div></section></section><div><div xml:lang="en-US" class="legalnotice" id="idm46144045553040"><h1 class="legalnotice">Legal Notice</h1><div class="para">
		Copyright <span class="trademark"/>© 2024 Red Hat, Inc.
	</div><div class="para">
		The text of and illustrations in this document are licensed by Red Hat under a Creative Commons Attribution–Share Alike 3.0 Unported license ("CC-BY-SA"). An explanation of CC-BY-SA is available at <a class="uri" href="http://creativecommons.org/licenses/by-sa/3.0/">http://creativecommons.org/licenses/by-sa/3.0/</a>. In accordance with CC-BY-SA, if you distribute this document or an adaptation of it, you must provide the URL for the original version.
	</div><div class="para">
		Red Hat, as the licensor of this document, waives the right to enforce, and agrees not to assert, Section 4d of CC-BY-SA to the fullest extent permitted by applicable law.
	</div><div class="para">
		Red Hat, Red Hat Enterprise Linux, the Shadowman logo, the Red Hat logo, JBoss, OpenShift, Fedora, the Infinity logo, and RHCE are trademarks of Red Hat, Inc., registered in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Linux</span>® is the registered trademark of Linus Torvalds in the United States and other countries.
	</div><div class="para">
		<span class="trademark">Java</span>® is a registered trademark of Oracle and/or its affiliates.
	</div><div class="para">
		<span class="trademark">XFS</span>® is a trademark of Silicon Graphics International Corp. or its subsidiaries in the United States and/or other countries.
	</div><div class="para">
		<span class="trademark">MySQL</span>® is a registered trademark of MySQL AB in the United States, the European Union and other countries.
	</div><div class="para">
		<span class="trademark">Node.js</span>® is an official trademark of Joyent. Red Hat is not formally related to or endorsed by the official Joyent Node.js open source or commercial project.
	</div><div class="para">
		The <span class="trademark">OpenStack</span>® Word Mark and OpenStack logo are either registered trademarks/service marks or trademarks/service marks of the OpenStack Foundation, in the United States and other countries and are used with the OpenStack Foundation's permission. We are not affiliated with, endorsed or sponsored by the OpenStack Foundation, or the OpenStack community.
	</div><div class="para">
		All other trademarks are the property of their respective owners.
	</div></div></div></div></div></div><script type="text/javascript">
                        jQuery(document).ready(function() {
                            initSwitchery();
                            jQuery('pre[class*="language-"]').each(function(i, block){hljs.highlightBlock(block);});
                        });
                    </script></body></html>